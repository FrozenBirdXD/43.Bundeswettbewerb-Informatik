\documentclass[a4paper,10pt,ngerman]{scrartcl}
\usepackage{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[a4paper,margin=2.5cm,footskip=0.5cm]{geometry}
\usepackage{listings}

\newcommand{\Aufgabe}{Aufgabe 1: Schmucknachrichten} 
\newcommand{\TeilnahmeId}{74130}                
\newcommand{\Name}{Matthew Greiner}           

\usepackage{scrlayer-scrpage, lastpage}
\setkomafont{pageheadfoot}{\large\textrm}
\lohead{\Name}
\chead{\Aufgabe}
\rohead{Teilnahme-ID: \TeilnahmeId}
\cfoot*{\thepage{}/\pageref{LastPage}}

\usepackage{titling}
\setlength{\droptitle}{-1.0cm}

\lstset{ %
  language=Java,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{green!50!black},
  stringstyle=\color{orange},
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=5pt,
  backgroundcolor=\color{white},
  showspaces=false,
  showstringspaces=false,
  tabsize=2,
  breaklines=true,
  breakatwhitespace=true,
  frame=single,
  captionpos=b,
}

\usepackage{etoolbox}
\usepackage{verbatim}
\makeatletter
\def\@xobeysp{\ }
\let\verbatim@nolig@list\empty
\appto\verbatim@font{\raggedright}
\makeatother

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{hyperref}
\usepackage{float}
\usepackage{graphicx}

\usepackage{algpseudocode}

\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstset{
  keywordstyle=\color{blue},commentstyle=\color{mygreen},
  stringstyle=\color{mymauve},rulecolor=\color{black},
  basicstyle=\footnotesize\ttfamily,numberstyle=\tiny\color{mygray},
  captionpos=b,
  keepspaces=true, 
  numbers=left, numbersep=5pt, showspaces=false,showstringspaces=true,
  showtabs=false, stepnumber=2, tabsize=2, title=\lstname
}
\lstdefinelanguage{JavaScript}{ 
  keywords={break, case, catch, continue, debugger, default, delete, do, else, finally, for, function, if, in, instanceof, new, return, switch, this, throw, try, typeof, var, void, while, with},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]",
  sensitive=true
}

\usepackage{cleveref}

\title{\textbf{\Huge\Aufgabe}}
\author{\LARGE Teilnahme-ID: \LARGE \TeilnahmeId \\\\
	    \LARGE Bearbeiter/-in dieser Aufgabe: \\ 
	    \LARGE \Name\\\\}
\date{\LARGE\today}

\begin{document}

\maketitle
\tableofcontents

\vspace{0.5cm}

\section{Teilaufgabe A}
\subsection{Lösungsidee}

\vspace{0.3cm}
\textbf{Verständnis der Aufgabe}
\newline
\newline
Die Grundidee dieser Aufgabe ist es einen gegebenen Unicode-Text mit Hilfe von Perlen zu kodieren, die alle denselben Durchmesser haben. Die Anzahl der Farben der Perlen ist gegeben und beträgt mindestens zwei. Das Ziel ist es, den gegebenen Text so mit den Perlen zu kodieren, dass eine möglichst kurze Perlenkette daraus gemacht werden kann. Die verwendete Codetabelle für die präfixfreie Kodierung und die Gesamtlänge der Botschaft muss am Ende ausgegeben werden.  \\
\newline
\textbf{Überlegungen zum Lösungsansatz}
\newline
\newline
Da der Durchmesser aller Perlen gleich ist, muss die Anzahl der verwendeten Perlen in der Kodierung minimiert werden, um die Perlenkette möglichst kurz zu halten. Da jedes Zeichen aus dem Eingabetext einzeln durch einen Code aus Perlen dargestellt werden muss, macht es Sinn, dass häufig verwendete Zeichen kurze Codes und selten verwendete Zeichen längere Codes bekommen. Diese Anforderungen eine
\textbf{optimale präfixfreie Kodierung basierend auf Zeichenhäufigkeiten zur Minimierung der Gesamtlänge}
ist eine klassische Anwendung der Huffman-Kodierung, die auch als Basis für die Lösung dieser Teilaufgabe verwendet wird. 
\newline
\newline
Die Huffman-Kodierung ist ein Algorithmus, der unter folgenden Bedingungen eine optimale binäre Kodierung liefert: 
\begin{enumerate}
  \item Jedes Zeichen bekommt einen eindeutigen Binärcode
  \item Die Nachricht ist verlustfrei und eindeutig dekodierbar
  \item Die durchschnittliche Länge (gewichtet nach Häufigkeit) wird minimiert
\end{enumerate}
Da diese Bedingungen genau zu dieser Aufgabe passen, eignet sich diese Kodierung für diese Aufgabe gut. Standardmäßig funktioniert die Huffman-Kodierung aber binär. Da wir die gegebene Nachricht evtl. aber mit mehr als zwei Farben von Perlen kodieren müssen, muss der Algorithmus auf eine \textbf{n-ären} Huffman-Kodierung erweitert werden. Eine solche Verallgemeinerung des Algorithmus wurde bereits im Originalpaper von Huffman \cite{paperhuffman} beschrieben.
\newline
Diese Erweiterung des Algorithmus funktioniert analog zu der binären Huffman-Kodierung, nur die Erstellung des Huffman-Baums muss angepasst werden. Bei der normalen Huffman-Kodierung werden zunächst die zu kodierenden Zeichen in Knoten gespeichert.
Dann werden die \textbf{zwei} Knoten mit den geringsten Häufigkeiten kombiniert und als Kinder von einem neuen Knoten gespeichert. Die “Häufigkeit” des neuen Knotens wird als Summe der beiden Häufigkeiten der Kinder festgelegt. Dieser Prozess wird wiederholt, bis nur noch ein Knoten übrig ist. Dieser ist dann die Wurzel des neuen Baums. Das bedeutet, bei jeder Kombinierung, verringert sich die Anzahl der betrachteten Knoten um eins. Der Baum wird also “bottom-up” erstellt, und garantiert somit eine optimale Kodierung (im Gegensatz zur Shannon-Fano-Kodierung, die einen Baum “top-down” erstellt und nicht immer die optimale Kodierung findet).  \\
\newline
Der Unterschied zu der \textbf{n-nären} Huffman-Kodierung ist nur, dass die $n$ Knoten mit den geringsten Häufigkeiten kombiniert werden und nicht nur zwei. Das hat zur Folge, dass mit jeder Kombination die Anzahl der betrachteten Knoten nun um $n-1$ sinkt. Daher muss gezielt darauf geachtet werden, dass nach allen Kombinationen nur noch ein Knoten übrigbleibt. Der Aufbau des n-nären Baums funktioniert, wenn folgende Gleichung erfüllt ist: 

\begin{equation}
N \bmod (n - 1) = 1 \quad \text{(}N = \text{Anzahl der Knoten}\text{)}
\label{eq:1}
\end{equation}

Wenn das nicht der Fall ist, können Platzhalterknoten hinzugefügt werden, die die Häufigkeit 0 haben, bis diese Bedingung erfüllt ist. 
Bei der Erstellung der Codetabelle wird der Graph (analog zur Standard Huffman-Kodierung) z.B. mit Tiefensuche traversiert. Dabei wird jeder Kante von einem Elternknoten zu einem Kindknoten eindeutig eine Zahl (z.B. von 0 bis k-1) zugeordnet. Eine Zahl repräsentiert in dieser Aufgabe eine Perlenfarbe. In der binären Version werden nur die Zahlen 0 und 1 benutzt. 


\subsection{Umsetzung}

Um diesen Algorithmus umzusetzen und somit die Aufgabe zu lösen, wird wie folgt vorgegangen (die Nummerierung entspricht auch der Nummerierung in den Quellcode): 
\begin{enumerate}
  \item \textbf{Einlesen des Eingabetextes und der Anzahl der verschiedenen Perlenfarben} \\
  Da alle Eingaben in Dateien vorliegen, welche nur 3 Zeilen haben, werden sie zunächst eingelesen und an den Zeilenumbrüchen getrennt. Der Text sowie die Anzahl der verschiedenen Perlenfarben werden anschließend in entsprechenden Variablen gespeichert. 
  \item \textbf{Analyse der Häufigkeit der Zeichen im Text} \\
  Um zu zählen, wie häufig die Zeichen in dem Text vorkommen, wird über alle Zeichen des Texts iteriert. Die Werte werden in einer HashMap gespeichert, wobei das Zeichen den Schlüssel und die Anzahl seines Vorkommens im Text den zugehörigen Wert darstellt. 
  \item \textbf{Erstellung einen n-nären Huffman-Baums basierend auf den Häufigkeiten} \\
  Um den Baum zu erstellen, wird eine Klasse Node definiert, die einen Knoten im Huffman-Baum repräsentiert. Diese Klasse speichert das Zeichen, seine Häufigkeit, eine Liste seiner Kindknoten und einen Marker, ob der Knoten ein Blatt im Baum ist. Die einzelnen Knoten werden in einer PriorityQueue verwaltet, welche in Java als Min-Heap implementiert ist. Dadurch stehen die Knoten mit der geringsten Häufigkeit immer oben in der Warteschlange und diese Datenstruktur eignet sich daher gut, da für die Erstellung des Baums jeweils die $n$ Knoten mit den \textbf{geringsten} Häufigkeiten kombiniert werden.\\
  Dies ist was den Huffman Algorithmus zu einem \textit{greedy} Algorithmus macht. Er trifft in jedem Schritt eine lokal optimale Entscheidung (kombiniere die $n$ unwahrscheinlichsten Knoten), die nachweislich zur globalen optimalen Lösung führt.\\
  \newline
  \textbf{Ablauf}
  \newline
  Zunächst werden die Zeichen des Text in Blattknoten gespeichert und in die PriorityQueue eingefügt. Um aus diesen Knoten den Huffman-Baum zu konstruieren, kann man Platzhalter-Knoten einführen (siehe 1.1), allerdings gibt es auch einen Weg diese zusätzlichen Knoten zu vermeiden: \\
  Statt Platzhalter mit der Häufigkeit 0 zu erzeugen, werden beim ersten Merging nicht zwingend $n$ Knoten kombiniert, sondern eine bestimmte Anzahl $r$. Dieses $r$ muss so gewählt werden, dass nach der ersten Kombination eine Gesamtanzahl an Knoten entsteht, mit der sich der Rest des Baums in gleichmäßigen $n$-ner Gruppen weiterbauen lässt.\\
  \newline
  Es gilt die Gleichung \ref{eq:1}.
  \newline
  Wenn eine $n$-närer Baum gefordert ist, ist \textbf{N} die Anzahl an Blattknoten in der Queue. Die Gleichung muss auch nach dem ersten Merge mit $r$ Knoten gelten:
  \begin{equation*}
     (N-r+1) \bmod (n-1) = 1
  \end{equation*}
  Nach $r$ umstellen ergibt:
  \begin{align*}
    N - r &\equiv 0 \mod(n - 1) \\
    \Rightarrow \quad r &= N \bmod(n - 1) 
  \end{align*}

  $r$ muss mindestens 2 sein, damit ein Merge Sinn macht, daher wird:
  \begin{equation}
    r = (N - 2) \bmod (n - 1) + 2
    \label{eq:2}
  \end{equation}
  in der Implementierung verwendet.
  \newline
  In der Implementierung wird $r$ also einmalig für den ersten Merge verwendet. Danach werden Gruppen zu je $n$ Knoten kombiniert, bis nur noch ein Knoten übrig ist, das dann die Wurzel des Huffman-Baums ist.
  \item \textbf{Erzeugen der Codetabelle anhand des Baums} \\
  Zur Speicherung der Codetabelle wird eine Hashmap verwendet, in der die Zeichen als Schlüssel hat und ihre zugehörige n-äre Huffman-Kodierung als Werte abgelegt sind. Die Werte werden dann schrittweise bei der Traversierung des Baums schrittweise aufgebaut. \\
  Um die Kodierung für jedes Zeichen zu finden, wird der Baum mit einer rekursiven Tiefensuche (DFS) durchlaufen. Dabei wird bei jedem rekursiven Aufruf der bisher zurückgelegte Pfad vom Wurzelknoten bis zum Aktuellen Knoten als String übergeben.  \\
  \newline
  Weil es sich um eine n-äre Huffman-Kodierung handelt, werden die Kanten des Baums durch die Ziffern $0$ bis $n-1$ beschrieben. Die Ziffern entsprechen allgemein formuliert, den Kodierungsbuchstaben, die in diesem Fall die jeweilige Perlenart bzw. Perlenfarbe darstellt. Bei jedem Abstieg zu einem Kindknoten wird an die aktuelle Zeichenkette die Position des jeweiligen Kindes im Elternknoten angehängt. Wenn ein Blattknoten erreicht wird, also ein Knoten, der tatsächlich ein Zeichen repräsentiert und keine weiteren Nachfolger besitzt, wird der aktuell aufgebaute Pfad als Kodierung für dieses Zeichen in der Codetabelle gespeichert. 

  \item \textbf{Berechnung der Gesamtlänge der kodierten Nachricht} \\
  Für die Berechnung der Gesamtlänge der kodierten Nachricht wird die Häufigkeit jedes Zeichens (bereits in Schritt 2 bestimmt) mit der Länge seines entsprechenden Huffman-Codes multipliziert. Die Gesamtlänge ergibt sich aus der Summe dieser Produkte multipliziert mit dem Durchmesser einer Perle.
  \item \textbf{Ausgabe der Codetabelle und der Gesamtlänge} \\
   Die Tabelle wird nach Code-Länge und Frequenz sortiert (optional, allerdings ein “nice to have” um direkt ablesen zu können, welche Zeichen am meisten vorkommen, wenn dies auch ausgegeben wird, was in dieser Implementierung gemacht wird). Anschließend wird sie formatiert ausgegeben sowie die Gesamtlänge der kodierten Nachricht aus 5. 
\end{enumerate}


\subsection{Komplexitätsanalyse}


Nun wird die Komplexität der einzelnen Schritte des Algorithmus betrachtet. Dabei ist:
\begin{itemize}
  \item $L$ die Länge des Eingabetextes,
  \item $k$ die Anzahl der verschiedenen Zeichen (in dieser Dokumentation auch als Quellsymbole bezeichnet),
  \item $n$ die Basis des Huffman-Baums (Anzahl erlaubter Kindknoten je Knoten).
\end{itemize}

\vspace{0.2cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Schritt} & \textbf{Laufzeit} & \textbf{Speicherbedarf} \\
\hline
Einlesen der Datei & $\mathcal{O}(L)$ & $\mathcal{O}(L)$ \\
Häufigkeitsanalyse & $\mathcal{O}(L)$ & $\mathcal{O}(k)$ \\
Aufbau des Huffman-Baums & $\mathcal{O}(k \log k)$ & $\mathcal{O}(k)$ \\
Generieren der Codetabelle & $\mathcal{O}(k)$ & $\mathcal{O}(k)$ \\
Berechnung der Gesamtlänge & $\mathcal{O}(k)$ & $\mathcal{O}(1)$ \\
Ausgabe Codetabelle (mit Sortierung) & $\mathcal{O}(k \log k)$ & $\mathcal{O}(k)$ \\
\hline
\textbf{Gesamt} & $\boldsymbol{\mathcal{O}(L + k \log k)}$ & $\boldsymbol{\mathcal{O}(L + k)}$ \\
\textbf{Dominante Terme} & $\small\textit{O(L) potentiell langer Text}$ & $\small\textit{}$ \\
\textbf{} & $\small\textit{O(k log k) wegen Heap Operation/Sortierung}$ & $\small\textit{}$ \\
\hline
\end{tabular}
\caption{Laufzeit- und Speicherkomplexität Teilaufgabe A}
\end{table}

\vspace{1em}

\begin{itemize}
  \item \textbf{Einlesen und Häufigkeitsanalyse:} Beide Vorgänge durchlaufen den Text vollständig und brauchen daher eine Laufzeit von $\mathcal{O}(L)$.
  
  \item \textbf{Baumkonstruktion:} Wegen der Prioritätswarteschlange (Min-Heap) mit $k$ Blattknoten entstehen $\frac{k - 1}{n - 1}$ Merges. Jede Insert- und Delete Operation im Heap braucht $\mathcal{O}(\log k)$, was insgesamt zu $\mathcal{O}(k \log k)$ führt.
  
  \item \textbf{Codetabelle:} Dieser Schritt ist durchläuft jeden Knoten ($k$ Knoten) genau einmal mit DFS (Tiefensuche). Pro Knoten wird eine Schleife über alle $n$ Kinder ausgeführt. Bei der n-nären Kodierung hat der Baum im schlimmsten Fall $\mathcal{O}(k)$ Knoten. Da $n$ eine kleine Konstante ist, beträgt die Laufzeit für diesen Schritt $\mathcal{O}(k)$.

  \item \textbf{Gesamtlänge:} Dieser Schritt iteriert über alle verschiedenen Symbole, die einen Code bekommen und ist daher in $\mathcal{O}(k)$.
  
  \item \textbf{Ausgabe:} Die Codetabelle wird nach Frequenz und Code-Länge sortiert, was im Worst Case ebenfalls $\mathcal{O}(k \log k)$ erfordert. (Ohne die optionale Sortierung wäre das das in $\mathcal{O}(k)$)
\end{itemize}

\vspace{1em}
Insgesamt ist die Laufzeit dominiert durch das Einlesen des Textes und die Sortiervorgänge beim Baumaufbau mit dem Min-Heap. Der Speicherbedarf ist moderat und wächst linear mit der Eingabegröße sowie der Anzahl unterschiedlicher Zeichen. 
\newline
Daher erweist sich dieser Algorithmus auch bei größeren Texten als leistungsfähig und garantiert eine optimale Kodierung.


\subsection{Beispiele}

Hier die Ausgaben des Programmes zu den Beispielen auf der BwInf-Webseite:

\begin{enumerate}
  \item \textbf{schmuck0.txt}
\begin{verbatim}
Codetabelle:
(Jede Ziffer steht für eine Perlenfarbe z.B. könnte '0' rot bedeuten und '1' blau)
{
  '␣': 111 (Freq: 5)
  'E': 110 (Freq: 5)
  'I': 100 (Freq: 4)
  'N': 101 (Freq: 4)
  'S': 000 (Freq: 3)
  'R': 0100 (Freq: 2)
  'D': 0010 (Freq: 2)
  'L': 0111 (Freq: 2)
  'M': 0110 (Freq: 2)
  'O': 0011 (Freq: 2)
  'C': 01010 (Freq: 1)
  'H': 01011 (Freq: 1)
}

Gesamtlänge der Botschaft 113 (Anzahl in Perlen) bzw. 11.3cm
\end{verbatim}


\item \textbf{schmuck00.txt}
\begin{verbatim}
Codetabelle:
(Jede Ziffer steht für eine Perlenfarbe z.B. könnte '0' rot bedeuten und '1' blau)
{
  '␣': 21 (Freq: 24)
  'e': 12 (Freq: 18)
  'i': 11 (Freq: 16)
  't': 02 (Freq: 10)
  'n': 01 (Freq: 9)
  'h': 220 (Freq: 8)
  's': 222 (Freq: 8)
  'c': 202 (Freq: 7)
  'l': 201 (Freq: 6)
  'a': 100 (Freq: 4)
  'r': 102 (Freq: 4)
  'd': 001 (Freq: 3)
  'D': 2212 (Freq: 3)
  'u': 2211 (Freq: 3)
  'G': 1012 (Freq: 2)
  'g': 2210 (Freq: 2)
  'm': 2000 (Freq: 2)
  'w': 2002 (Freq: 2)
  'E': 0001 (Freq: 1)
  'P': 0020 (Freq: 1)
  'W': 0002 (Freq: 1)
  'Z': 1010 (Freq: 1)
  'b': 0022 (Freq: 1)
  'f': 1011 (Freq: 1)
  'k': 0021 (Freq: 1)
  'o': 0000 (Freq: 1)
  'A': 20010 (Freq: 1)
  '?': 20011 (Freq: 1)
}

Gesamtlänge der Botschaft 372 (Anzahl in Perlen) bzw. 37.2cm
\end{verbatim}

\item \textbf{schmuck01.txt}
\begin{verbatim}
Codetabelle:
(Jede Ziffer steht für eine Perlenfarbe z.B. könnte '0' rot bedeuten und '1' blau)
{
  '␣': 2 (Freq: 85)
  'e': 0 (Freq: 66)
  'n': 43 (Freq: 53)
  'r': 41 (Freq: 34)
  'i': 40 (Freq: 33)
  's': 33 (Freq: 26)
  'a': 31 (Freq: 22)
  'l': 32 (Freq: 22)
  't': 30 (Freq: 22)
  'h': 13 (Freq: 17)
  'c': 12 (Freq: 15)
  'g': 11 (Freq: 14)
  'o': 10 (Freq: 14)
  'm': 443 (Freq: 13)
  'u': 442 (Freq: 12)
  '.': 441 (Freq: 10)
  'd': 424 (Freq: 9)
  'k': 440 (Freq: 9)
  'E': 423 (Freq: 8)
  ',': 422 (Freq: 8)
  'D': 344 (Freq: 7)
  'b': 421 (Freq: 7)
  'w': 343 (Freq: 6)
  'ü': 341 (Freq: 5)
  'I': 340 (Freq: 4)
  'S': 144 (Freq: 4)
  'f': 143 (Freq: 4)
  'p': 142 (Freq: 4)
  'v': 141 (Freq: 4)
  'V': 140 (Freq: 3)
  'F': 4442 (Freq: 3)
  'O': 4443 (Freq: 3)
  'ö': 4441 (Freq: 3)
  'z': 4444 (Freq: 3)
  'B': 4440 (Freq: 2)
  'G': 4203 (Freq: 2)
  'ä': 4204 (Freq: 2)
  'H': 3420 (Freq: 1)
  'K': 3421 (Freq: 1)
  'M': 3422 (Freq: 1)
  'N': 3424 (Freq: 1)
  'R': 4200 (Freq: 1)
  'W': 4202 (Freq: 1)
  'ß': 3423 (Freq: 1)
  '…': 4201 (Freq: 1)
}

Gesamtlänge der Botschaft 1150 (Anzahl in Perlen) bzw. 115.0cm
\end{verbatim}
\end{enumerate}

\subsection{Quellcode}
\textbf{Wichtige Teile des Quellcodes:}\\
\newline
\vspace{0.5em}
\textbf{1. Einlesung der Daten}
\begin{lstlisting}
// 1. Read input
private static InputWrapper parseInput(String input) {
    // Save every line in array of Strings
    String[] lines = input.split("\n");
    List<Integer> list = new ArrayList<>();
    for (String s : lines[1].split(" ")) {
        list.add(Integer.valueOf(s));
    }
    return new InputWrapper(Integer.parseInt(lines[0].trim()), lines[2], list);
}
\end{lstlisting}

\textbf{2. Häufigkeitsanalyse}
\begin{lstlisting}
// 2. Calculate character frequency from input text
private static Map<Character, Long> buildFrequencyMap(String text) {
    Map<Character, Long> frequencyMap = new HashMap<>();
    for (char character : text.toCharArray()) {
        frequencyMap.put(character, frequencyMap.getOrDefault(character, 0L) + 1);
    }
    return frequencyMap;
}
\end{lstlisting}


\textbf{3. Erstellung einen n-nären Huffman-Baums}
\begin{lstlisting}
// 3. Build n-ary Huffman tree and returns root Node
private static Node buildHuffmanTree(Map<Character, Long> frequencyMap, int n) {
    // PriorityQueue for nodes (sorted by frequency, lower frequency first)
    PriorityQueue<Node> priorityQueue = new PriorityQueue<>();

    // Create leaf nodes (nodes with characters) and add to priority queue
    for (Map.Entry<Character, Long> entry : frequencyMap.entrySet()) {
        priorityQueue.add(new Node(entry.getKey(), entry.getValue()));
    }

    int numSymbols = priorityQueue.size(); // Number of unique symbols (leaf nodes)

    // Determine how many nodes should be merged in the first step
    int r;
    if (numSymbols <= 1) { // Handle edge case: if 0 or 1 symbol, no merging needed
        r = numSymbols;
    } else {
        // Formula: find smallest valid r so that:
        // After merging r nodes -> remaining nodes + 1 new node
        // -> total node count allows full n-ary merges
        int remainder = (numSymbols - 2) % (n - 1);
        r = remainder + 2; // Ensures 2 <= r <= n
    }

    // Build Huffman tree
    while (priorityQueue.size() > 1) {
        // Use r only for first merge if needed
        int nodesToMerge = (priorityQueue.size() == numSymbols) ? r : n;
        // Cannot merge more nodes than available
        nodesToMerge = Math.min(nodesToMerge, priorityQueue.size()); 

        if (nodesToMerge < 2) {
            // Should not happen if n > 1
            System.out.println("irgendwas ist broken");
            break;
        }

        List<Node> children = new ArrayList<>();
        long mergedFrequency = 0;

        // Extract nodes (num of nodesToMerge) with lowest frequencies
        for (int i = 0; i < nodesToMerge; i++) {
            Node node = priorityQueue.poll();
            children.add(node);
            mergedFrequency += node.frequency;
        }

        // Create new internal node with new values
        Node internalNode = new Node(children, mergedFrequency);

        // Add new internal node back to priority queue
        priorityQueue.add(internalNode);
    }

    // Last node in queue is root of Huffman tree
    return priorityQueue.poll();
}
\end{lstlisting}

\textbf{4. Erzeugen der Codetabelle}
\begin{lstlisting}
// 4. Generate Huffman codes by traversing tree with recursion and DFS
private static void generateCodes(Node node, String currentCode, Map<Character, String> codeTable) {
    // Base case
    if (node == null) {
        return;
    }

    // If it is a leaf, it represents a character, then assign a code
    if (node.isLeaf()) {
        if (node.character != null) {
            codeTable.put(node.character, currentCode);
        }
        // Leaf is reached, stop recursion
        return;
    }

    // Assign codes 0 until k-1 to children
    for (int i = 0; i < node.children.size(); i++) {
        generateCodes(node.children.get(i), currentCode + i, codeTable);
    }
}
\end{lstlisting}

\textbf{5. Berechnung der Gesamtlänge}
\begin{lstlisting}
// 5. Calculate total length of encoded message
private static long calculateTotalLength(Map<Character, String> codeTable, Map<Character, Long> frequencyMap,
        int diameter) {
    long totalLength = 0;
    // Iterate over each Character
    for (Map.Entry<Character, Long> entry : frequencyMap.entrySet()) {
        totalLength += entry.getValue() * codeTable.get(entry.getKey()).length();
    }
    return totalLength * diameter;
}
\end{lstlisting}

\textbf{6. Ausgabe}
\begin{lstlisting}
// Print code table in a readable format
private static void printCodeTable(Map<Character, String> codeTable, Map<Character, Long> frequencyMap) {
    if (codeTable.isEmpty()) {
        System.out.println("{}");
        return;
    }
    List<Character> sortedKeys = new ArrayList<>(codeTable.keySet());
    // Sorting is optional
    // Sort by length of code
    sortedKeys.sort((a, b) -> Integer.compare(codeTable.get(a).length(), codeTable.get(b).length()));
    // Sort by frequency
    sortedKeys.sort((a, b) -> Long.compare(frequencyMap.get(b), frequencyMap.get(a)));

    System.out.println("{");
    for (Character character : sortedKeys) {
        // Handle special characters for printing
        String c;
        c = switch (character) {
            case ' ' -> " ";
            case '"' -> c = "\"";
            default -> String.valueOf(character);
        };
        System.out.println("  '" + c + "': " + codeTable.get(character) + " (Freq: "
                + frequencyMap.get(character) + ")");
    }
    System.out.println("}");
}
\end{lstlisting}


\section{Teilaufgabe B}
\subsection{Lösungsidee}
\textbf{Verständnis der Aufgabe}\\
\newline
Die Grundidee dieser Aufgabe bleibt gleich: Ein Unicode-Text soll mit Hilfe von Perlen kodiert werden. Allerdings haben die Perlen nun einen unterschiedlichen Durchmesser. Ziel ist es nun, die Gesamtlänge der resultierenden Perlenkette, also die kodierte Nachricht, zu minimieren.  \\
Wenn dieses Problem weiterhin als Optimierungsproblem zu Erstellung einer optimalen, präfixfreien Kodierung (wie bereits im ersten Teil der Aufgabe) betrachtet wird, führt die Einführung variabler Perlendurchmesser zu einer Verallgemeinerung des ursprünglichen Problems. Denn in dieser Erweiterung entspricht der Durchmesser jeder Perle den Kosten des jeweiligen Symbols im Codealphabet. Im Folgenden wird einfachheitshalber, die Perle als Symbol bezeichnet, da diese verwendet werden, um den Text zu kodieren und der Durchmesser der Perle wird als Kosten des Symbols bezeichnet. \\
\newline
Diese Variante ist bekannt als das Problem der \textbf{präfixfreien Kodierung mit ungleichen Symbolkosten} (prefix-free coding with unequal letter cost). Im Gegensatz zur klassischen Huffman-Kodierung, die von gleichen Symbolkosten ausgeht, ist diese Verallgemeinerung deutlich komplexer. Es ist bis heute \textbf{nicht bekannt}, ob dieses allgemeine Problem in \textbf{P} liegt oder \textbf{NP-schwer} ist. Allerdings gibt es einen (komplexen) Algorithmus, der die optimale Kodierung findet, wenn die Symbolkosten postive ganze Zahlen sind. Diese Vereinfachung trifft auf diese Teilaufgabe zu, weshalb dieser Algorithmus in Kapitel 2.4 noch eingeführt wird. Somit ist Teilaufgabe B in polynomieller Zeit lösbar.
\\\newline
\textbf{Überlegungen zum Lösungsansatz}\\
\newline
Die aus Kapitel 1 implementierte n-näre Huffman-Kodierung scheitert bei dieser Problemstellung, da die Minimierung der Anzahl der Kodierungsbuchstaben nicht notwendigerweise zur Minimierung der Gesamtkosten führt (die Kosten werden nämlich nicht berücksichtigt).  
\newline
Im Folgenenden werden drei Strategien zur Lösung dieses Problems untersucht: 

\subsection{Strategie 1: Heuristische n-näre Huffman Modifikation mit greedy Kostenzuweisung}
\subsubsection{Kernidee}
Dieser Ansatz ist vermutlich der Intuitivste und am wenigsten aufwendige und ist in zwei Schritte aufgeteilt: 
\\\newline
\begin{enumerate}
  \item \textbf{Erstellung eines n-nären Huffman Baums} \\
      Zunächst wird ein n-närer Huffman mit dem aus Kapitel 1 beschriebenen unveränderten Algorithmus erstellt, bei der der Baum nur basierend auf den Symbolhäufigkeiten gebaut wird. Dieser Schritt ignoriert also die Symbolkosten und bestimmt nur die Topologie des Baums. 
  \item \textbf{Greedy Zuweisung der Kodierungsbuchstaben anhand der Kosten}\\
  Bei der Traversierung des Baums zur Erstellung der Codewörter erfolgt die Beschriftung der Kanten (von inneren Knoten mit $n$ Kindern) standardmäßig mit den Ziffern bzw. Kodierungszeichen $0$ bis $n-1$.\\
  Während bei der klassischen Methode aus Kapitel 1 diese Zuweisung oft der Reihenfolge der Kinder folgt (z.B. von links nach rechts), ändert sich dies in dieser Heuristik:\\
  Denn nun erfolgt diese Zuweisung greedy nach folgendem Prinzip: \\
  Die Kante, die zum Teilbaum mit der höchsten Häufigkeit führt, erhält jetzt den Kodierungsbuchstaben mit den geringsten Kosten. Die zweithäufigste Kante erhält das zweigünstigste Symbol usw. Dadurch wird angestrebt, dass teure Kodierungszeichen, also Perlen mit großem Durchmesser möglichst selben verwendet werden (in Pfaden mit seltenen Symbolen). So werden günstige Zeichen möglichst früh im Baum platziert, sodass sie häufiger vorkommen und damit mehr zur Gesamtkodierung beitragen können.\\
\end{enumerate}
Diese lokale Regel zur Zuordnung führt nicht zu einer global optimalen Lösung, aber sie verteilt die teuren Kosten möglichst kosteneffizient in dem bestehenden Baum. Diese Heuristik ist leicht implementierbar, da diese nur eine kleine Anpassung der n-nären Huffman-Kodierung benötigt. Aus diesen Gründen wurde diese Strategie als erster Ansatz für die Lösung dieses Teilproblems gewählt. Sie dient als Baseline, an der weiterführende Verfahren verglichen werden können, die diese Strategie idealerweise übertreffen. 
\subsubsection{Umsetzung}
Die Umsetzung dieser Strategie erfordert nur eine Anpassung der Teilschritte 4 und 5 des beschriebenen Algorithmus aus Kapitel 1. Die Schritte 1 bis 3 bleiben identisch (bis auf die Einlesung der Daten in Schritt 1 die minimal angepasst wird). Folgendes wird angepasst:

\begin{enumerate}
  \item [4.] \textbf{Traversierung mit greedy Kostenzuweisung}\\
  Die eigentliche Traversierung des Baums mit der rekursiven Tiefensuche bleibt identisch. Für die Zuweisung der Kosten, werden zunächst die Kinder nach ihrer Häufigkeit absteigend sortiert. Die verfügbaren Kodierungssymbole (in diesem Fall die Ziffern $0$ bis $n-1$ ), werden auch gemäß ihren zugehörigen Kosten aufsteigend sortiert. Mit einer Schleife über alle Kinder, wird den Kanten zu den Teilbäumen nun jeweils das günstigste noch verfügbare Symbol zugewiesen, beginnend mit dem häufigsten Teilbaum. Für jedes Kind wird dieser Prozess rekursiv wiederholt, wobei der aktuelle Pfad (die bisher zugewiesenen Kodierungssymbole) jeweils an das entstehende Codewort angehängt wird. 
  \item[5.] \textbf{Berechnung der Gesamtlänge der kodierten Nachricht}\\
  In diesem Schritt wird die Gesamtlänge der kodierten Nachricht bestimmt. Im Unterschied zur klassischen Huffman-Kodierung, bei der die Länge eines Codeworts einfach durch die Anzahl der Symbole bestimmt wird, müssen hier die individuellen Kosten der verwendeten Kodierungssymbole mit einberechnet werden. Die Gesamtlänge ergibt sich also nicht nur aus der Anzahl der Symbole im Codewort, sondern aus der Summe der Kosten aller Symbole, die zur Kodierung eines Zeichens verwendet werden. 
\end{enumerate}


\subsubsection{Komplexitätsanalyse}
Die Erklärung für die unveränderten Teilschritten bleibt wie in den Kapitel 1 und 2.2 gleich. Die Komplexität des Algorithmus has sich wie folgt, geändert:

\vspace{0.2cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Schritt} & \textbf{Laufzeit} & \textbf{Speicherbedarf} \\
\hline
Einlesen der Datei & $\mathcal{O}(L)$ & $\mathcal{O}(L)$ \\
Häufigkeitsanalyse & $\mathcal{O}(L)$ & $\mathcal{O}(k)$ \\
Aufbau des Huffman-Baums & $\mathcal{O}(k \log k)$ & $\mathcal{O}(k)$ \\
\textbf{Generieren der Codetabelle} & $\mathcal{O}(k \cdot n \log n)$ & $\mathcal{O}(k \cdot n)$ \\
\textbf{Berechnung der Gesamtlänge} & $\mathcal{O}(k)$ & $\mathcal{O}(1)$ \\
Ausgabe Codetabelle (mit Sortierung) & $\mathcal{O}(k \log k)$ & $\mathcal{O}(k)$ \\
\hline
\textbf{Gesamt} & $\boldsymbol{\mathcal{O}(L + k \log k)}$ & $\boldsymbol{\mathcal{O}(L + k)}$ \\
\hline
\end{tabular}
\caption{Laufzeit- und Speicherkomplexität Teilaufgabe B Strategie 1}
\end{table}

\vspace{1em}

\begin{itemize}
  \item \textbf{Codetabelle:} Dieser Schritt ist durchläuft jeden Knoten (insgesamnt $k$ Knoten) genau einmal mit DFS (Tiefensuche). Pro Knoten werden die Kinder und Kodierungszeichen sortiert, was in $\mathcal{O}(n \log n)$ ist. Daher beträgt die Laufzeit für diesen Schritt $\mathcal{O}(k \cdot n \log n)$.

  \item \textbf{Gesamtlänge:} Dieser Schritt iteriert über alle verschiedenen Symbole, die einen Code bekommen. Es wird über jedes Zeichen in Code iteriert und somit ist dieser Schritt in $\mathcal{O}(k \cdot L_{max})$. $L_{max}$ ist hier die maximale Code-Länge (konstant), daher $\mathcal{O}(k)$.
\end{itemize}
\subsubsection{Beispiele}
Hier die Ausgaben des Programmes zu den Beispielen auf der BwInf-Webseite. Mit Ausnahme der Datei \textit{schmuck5.txt} (für die ein offizieller Vergleichswert vorliegt), wurde die Konsolenausgabe auf die Gesamtlänge der kodierten Nachricht reduziert. Dies wurde gemacht für eine klarere Vergleichbarkeit der Ergebnisse zwischen den noch folgenden Algorithmen. \\Die gesamte Ausgabe ist hier \textbf{AusgabenBStrategie1.txt} \\ \newline \textbf{EDIT} \\ \newline zufinden. 
Die Ausgabe wird zu dem finalen (und besten) Programm ausführlicher gegeben. 

\begin{enumerate}
  \item \textbf{schmuck1.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 197 (Anzahl in Perlen) bzw. 19.7cm
\end{verbatim}
  \item \textbf{schmuck2.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 145 (Anzahl in Perlen) bzw. 14.5cm
\end{verbatim}
  \item \textbf{schmuck3.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 279 (Anzahl in Perlen) bzw. 27.9cm
\end{verbatim}
  \item \textbf{schmuck4.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 154 (Anzahl in Perlen) bzw. 15.4cm
\end{verbatim}
  \item \textbf{schmuck5.txt}
\begin{verbatim}
Codetabelle:
(Jede Ziffer steht für eine Perlenfarbe z.B. könnte '0' rot bedeuten und '1' blau)
{
  '␣': 2 (Freq: 151, Länge: 2)
  'e': 3 (Freq: 110, Länge: 3)
  't': 4 (Freq: 71, Länge: 4)
  'i': 5 (Freq: 67, Länge: 5)
  'o': 00 (Freq: 64, Länge: 2)
  's': 01 (Freq: 61, Länge: 2)
  'a': 02 (Freq: 58, Länge: 3)
  'n': 03 (Freq: 56, Länge: 4)
  'r': 04 (Freq: 51, Länge: 5)
  'c': 05 (Freq: 44, Länge: 6)
  'd': 06 (Freq: 37, Länge: 7)
  'l': 10 (Freq: 33, Länge: 2)
  'm': 11 (Freq: 27, Länge: 2)
  'h': 12 (Freq: 26, Länge: 3)
  'u': 14 (Freq: 25, Länge: 5)
  'f': 16 (Freq: 20, Länge: 7)
  'p': 15 (Freq: 20, Länge: 6)
  'b': 60 (Freq: 16, Länge: 7)
  'y': 61 (Freq: 13, Länge: 7)
  'g': 62 (Freq: 11, Länge: 8)
  '.': 63 (Freq: 8, Länge: 9)
  'q': 66 (Freq: 5, Länge: 12)
  ',': 130 (Freq: 5, Länge: 5)
  'v': 131 (Freq: 5, Länge: 5)
  'x': 132 (Freq: 5, Länge: 6)
  'w': 133 (Freq: 4, Länge: 7)
  'F': 134 (Freq: 3, Länge: 8)
  'T': 135 (Freq: 3, Länge: 9)
  'A': 650 (Freq: 1, Länge: 12)
  'G': 644 (Freq: 1, Länge: 14)
  'H': 651 (Freq: 1, Länge: 12)
  'S': 640 (Freq: 1, Länge: 11)
   '’': 645 (Freq: 1, Länge: 15)
  'j': 652 (Freq: 1, Länge: 13)
  '-': 643 (Freq: 1, Länge: 13)
  '1': 646 (Freq: 1, Länge: 16)
  '2': 136 (Freq: 1, Länge: 10)
  '5': 653 (Freq: 1, Länge: 14)
  '9': 654 (Freq: 1, Länge: 15)
  'z': 641 (Freq: 1, Länge: 11)
  ';': 642 (Freq: 1, Länge: 12)
}

Gesamtlänge der Botschaft 4010 (Anzahl in Perlen) bzw. 401.0cm
\end{verbatim}

Dieses Ergebnis (4010) ist ca. $26.82\%$ schlechter als der Vergleichswert (3162) von der BwInf Webseite, daher ist hier noch viel Platz für Verbesserung.
  \item \textbf{schmuck6.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 244 (Anzahl in Perlen) bzw. 24.4cm
\end{verbatim}
  \item \textbf{schmuck7.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 153144 (Anzahl in Perlen) bzw. 15314.4cm
\end{verbatim}
  \item \textbf{schmuck8.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 3615 (Anzahl in Perlen) bzw. 361.5cm
\end{verbatim}
  \item \textbf{schmuck9.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 41056 (Anzahl in Perlen) bzw. 4105.6cm
\end{verbatim}
Dieses Ergebnis (41056) ist ca. $12.18\%$ schlechter als der Vergleichswert (36597) von der BwInf Webseite, daher ist hier noch etwas Platz für Verbesserung.
\end{enumerate}
\subsubsection{Quellcode}
\textbf{Wichtige Teile des Quellcodes:}\\

\textbf{4. Erzeugen der Codetabelle mit greedy Heuristik}
\begin{lstlisting}
// 4. Generate Huffman codes with cost-aware digit assignment
private static void generateCodesCostAware(Node node, String currentCode, Map<Character, String> codeTable,
        List<Integer> costs) {
    // Base case
    if (node == null) {
        return;
    }

    // If it is a leaf, it represents a character, then assign a code
    if (node.isLeaf()) {
        if (node.character != null) {
            codeTable.put(node.character, currentCode.isEmpty() ? "0" : currentCode);
        }
        // Leaf is reached, stop recursion
        return;
    }

    List<Node> children = node.children;

    // 1. Sort children by frequency - Descending (made copy to not modify the main
    // list, not sure if needed)
    List<Node> sortedChildren = new ArrayList<>(children);
    sortedChildren.sort(Comparator.comparingLong(Node::getFrequency).reversed());

    // 2. Prepare digits sorted by cost - Ascending
    // DigitCost is a wrapper to map each digit with its corresponding cost
    List<DigitCost> digitCosts = new ArrayList<>();

    for (int i = 0; i < costs.size(); i++) {
        digitCosts.add(new DigitCost(i, costs.get(i)));
    } 
    // Sort by cost ascending, for this input not needed as digits are already sorted
    Collections.sort(digitCosts);

    // 3. Assign cheapest digits to most frequent children
    for (int i = 0; i < sortedChildren.size(); i++) {
        Node child = sortedChildren.get(i);
        int assignedDigit = digitCosts.get(i).digit; // i-th cheapest digit
        generateCodesCostAware(child, currentCode + assignedDigit, codeTable, costs);
    }
}
\end{lstlisting}
\textbf{5. Berechnung der Gesamtlänge}

\begin{lstlisting}
// 5. Calculate total cost of encoded message
private static long calculateTotalCost(Map<Character, String> codeTable, Map<Character, Long> frequencyMap,
        List<Integer> costs) {
    long totalCost = 0;

    // Iterate over each character
    for (Map.Entry<Character, Long> entry : frequencyMap.entrySet()) {
        char character = entry.getKey();
        long frequency = entry.getValue();
        String code = codeTable.get(character);

        long costOfCode = 0;
        for (char digitChar : code.toCharArray()) {
            int digit = Character.getNumericValue(digitChar);
            costOfCode += costs.get(digit);
        }
        totalCost += frequency * costOfCode;
    }
    return totalCost;
}
\end{lstlisting}

\subsection{Strategie 2: Modifizierung Golin/Li Algorithmus}
\subsubsection{Kernidee}
Da die erste implementiere Strategie als Referenz verwendet wird und die berechneten Gesamtlängen der kodierten Nachrichten nicht nahe genug an den Werten der BwInf Webseite liegen, wird eine weiterer Algorithmus erkundet. Das Problem einen Text nahezu optimal präfixfrei zu kodieren, wenn die Symbole zur Kodierung unterschiedliche Kosten aufweisen, ist nicht trivial. Der Algorithmus von Golin und Li \cite{papergolinli} bietet eine effiziente $\mathcal{O}(n \log n)$ Lösung (mit $n$ als Anzahl der Quellsymbole) mit einer bewiesenen guten Fehlerschranke gegenüber dem Optimum, die besser als der Ansatz aus Kapitel 2.2 sein sollte. Daher wird im Folgenden ein Algorithmus basierend auf \cite[Fig 6. und Kapitel 4]{papergolinli} implementiert, der die Kernprinzipien des Golin/Li-Algorithmus anwendet, um einen solchen Präfixcode zu finden. Der umgesetzte Algorithmus vereinfacht allerdings Teile aus dem Paper etwas, da diese sehr komplex ist. Daher kann die bewiesene Fehlerschranke zu dem Optimum nicht mehr garantiert werden. Dennoch sollte der Algorithmus bessere Resultate bringen. 
\newline
Zum Verständnis des Algorithmus werden folgende zwei Hauptdatensätze definiert:
\begin{enumerate}
  \item Eine Menge von $n$ Quellsymbolen, jeweils mit einer zugehörigen Auftrittswahrscheinlichkeit $0 < p < 1$, basierend auf deren Häufigkeit im Text. Diese Symbole werden absteigend nach ihrer Wahrscheinlichkeit sortiert ($p_{1} \ge p_{2} \ge ... \ge p_{n}$). 
  \item Eine Menge von $t$ Kodierungsbuchstaben (Zielalphabet, also Perlenfarben bzw. Ziffern), jeweils mit positiven Kosten $c_{m}$ (also Durchmesser der Perle). Diese werden aufsteigend nach ihren Kosten sortiert ($c_{1} \le c_{2} \le ... \le c_{t}$)
\end{enumerate}
Ein wichtiger Aspekt zur Lösung des Problems ist, die unterschiedlichen Kosten der verfügbaren Kodierungs- buchstaben fair den Quellsymbolen zuzuordnen, welche unterschiedlich oft auftreten. Anstatt wie der klassische Huffman-Algorithmus das Zusammenfügen der unwahrscheinlichsten Symbole und dann eine Zuordnung mit Kodierungsbuchstaben, wählt Golin/Li einen globaleren Ansatz, der auf eine intelligente Aufteilung des Wahrscheinlichkeitsraums basiert. 
\newline
Die Kernidee des Algorithmus ist eine rekursive Aufteilung der Quellsymbole in sogenannte “Bins” bzw. Gruppen. Die Zuweisung des ersten Kodierungsbuchstabens zu verschiedenen Gruppen von Quellsymbolen soll dabei nicht willkürlich, sondern kostenbewusst gemacht werden. Man möchte erreichen, dass Kodierungs- buchstaben mit \textit{geringen} Kosten tendenziell den Gruppen von Quellsymbolen zugeordnet werden, die zusammen eine höhere Gesamtwahrscheinlichkeit aufweisen. Dies ist intuitiv. 
\newline
Um dieses Ausbalancieren zu erreichen, betrachtet der Algorithmus nicht die einzelnen Symbole isoliert, sondern ihre Verteilung im \textbf{kumulativen Wahrscheinlichkeitsraum}. an kann sich vorstellen, dass alle Quellsymbole, sortiert nach ihrer Wahrscheinlichkeit $p_{m}$ auf einem Intervall von $0$ bis $1$ angeordnet sind, wo die Länge des Abschnitts für Symbol $i$ seiner Wahrscheinlichkeit $p_{m}$ entspricht. Die Gesamtlänge dieses Intervalls ist $1$. \\
\newline
Der Algorithmus zerlegt nun dieses Gesamtintervall (bzw. in rekursiven Schritten ein Teilintervall, einer bereits gebildeten Gruppe) in $t$ Segmente, wo $t$ die Anzahl der verfügbaren Kodierungsbuchstaben ist. Jedes Segment wird einem Kodierungsbuchstaben $m$ zugeordnet und stellt ein Bin dar.
\newline
Nun ist das Entscheidende, wie groß diese Bins/Segmente gemacht werden. Hierfür wird die \textbf{charakteristische Wurzel c} benutzt. Sie ist gewissermaßen ein Skalierungsfaktor, der die Kosten $c_{m}$ von jedem Kodierungs- buchstaben in ein “effektives Gewicht” (definiert als $2^{-c\cdot c_{m}})$ übersetzt. Dieses Gewicht repräsentiert den “fairen Anteil”, die dieser Buchstabe in der Kodierung beiträgt. Die Breite des Bins für den Kodierungs- buchstaben $m$ wird nun proportional zu diesem effektiven Gewicht festgelegt: \\
\newline
\begin{equation}
  \text{Breite(Bin } m \text{)} = w \text{ (Gesamtwahrscheinlichkeit der aktuellen Symbolgruppe)} \cdot 2^{-c \cdot c_{m}}
\end{equation}

Diese charakteristische Wurzel $c$ ist definiert als eindeutige positive reelle Zahl, die die Gleichung mit $t$: Zahl der Kodierungssymbolen und $c_{m}$ die Kosten des $m$-ten Symbols. 
\begin{equation}
  \sum_{m=1}^{t}2^{-c \cdot c_{m}} = 1
  \label{eq:4}
\end{equation}
erfüllt. Diese Gleichung ist eine Verallgemeinerung der Kraft-McMillan-Ungleichung für den Fall ungleicher Kosten und ist eine notwendige Bedingung für die Existenz eines vollständigen Präfixcodes. \\
\newline
Aus der Definition von $c$ folgt, dass die Summe dieser Gewichte 1 betragen und diese Tatsache ist auch mathematisch notwendig, um den gesamten Wahrscheinlichkeitsraum der Quellsymbole abzudecken. In dem Fall mit den Perlen heißt das konkret, dass eine Perle mit großem Durchmesser durch das $c$ ein kleines “Gewicht”, also einen \textbf{kleineren Anteil} und eine kleine Perle ein großes Gewicht, also einen \textbf{größeren Anteil} am Wahrscheinlichkeitsintervall bekommt. \\
\newline
Nachdem das Wahrscheinlichkeitsintervall mit den gewichteten Kosten im Blick, aufgeteilt wurde, werden die einzelnen Quellsymbole diesen Bins zugeordnet. Dies geschieht mit einer Heuristik: Für jedes Symbol $p_i$ wird sein \textbf{Wahrscheinlichkeitsmittelpunkt} 
\begin{equation}
  s_i = P_{i - 1} + \frac{p_i}{2}
  \label{eq:5}
\end{equation}
betrachtet. Das Symbol wird dem Bin $m$ zugewiesen, in dessen Intervall $[L_m, R_m)$ dieser Mittelpunkt fällt. Dieser Prozess des Zuweisens von Symbolen an Bins wird \textbf{rekursiv} für jede dieser Bins wiederholt, um den zweiten Kodierungsbuchstaben zu bestimmen usw. In jedem rekursiven Schritt wird das relevante Teilintervall der Wahrscheinlichkeit erneut nach dem gleichen Prinzip (also kosten-gewichtet mit $c$) aufgeteilt. Dadurch entsteht ein Codebaum von dem die Struktur die ungleichen Kosten \textbf{von Anfang an berücksichtigt}, im Gegensatz zu n-närem Huffman. Daher sollte dieser Algorithmus bessere Ergebnisse liefern, die aber noch garantiert nicht optimal sind. Dieser Baum muss am Ende nicht mehr traversiert werden, da der Baum “top-down” (im Gegensatz zu Huffman) erstellt wird, und wenn der rekursive Aufruf zur Erstellung des Baums an ein “Blatt” (hier also wenn in einem Bin nur ein Symbol ist), wird der sich der Pfad bis dahin gemerkt und als Codewort des Symbols festgelegt. \\
\newline
Dieser Aufteilungsmechanismus funktioniert und ist der Kern dieser Strategie (2.3), weil er systematisch die \textbf{Wahrscheinlichkeiten der Quellsymbole mit den Kosten der Kodierungsbuchstaben verknüpft}. Die wahrscheinlicheren Symbole (die größere Abschnitte im kumulativen Intervall belegen) haben eine höhere Chance, in den breiteren Bins zu landen. Da die breiteren Bins durch die $c$-gewichtete Aufteilung den kostengünstigeren Kodierungsbuchstaben entsprechen, bekommen die häufigen Symbolgruppen tendenziell die “billigeren” Kodierungsbuchstaben.\\
\newline
Anmerkung: Die Mittelpunkt Heuristik kann auch zu suboptimalen Verteilungen führen, sodass es z.B. leere Bins zwischen gefüllten Bins sind oder, dass alle Symbole in Bin eins sind. Das Golin/Li Paper definiert sogenannte “Left Shift” und “Right Shift” Operationen, um diese Verteilungen zu verbessern. Der Left Shift füllt leere Bins auf, indem Symbola aus den nachfolgende Bins nach vorne verschoben werden. Dieser Prozess ist in dem Paper allerdings sehr komplex und wird in dieser Implementierung stark vereinfacht. Der Right Shift behandelt den seltenen Spezialfall, dass alle Symbole im ersten Bin landen, indem das letzte Symbol in den zweiten Bin zu verschoben wird. So wird eine übermäßige Nutzung des ersten Bins vermieden. 

\subsubsection{Umsetzung}
Diese Implementierung folgt den Kernprinzipien des Algorithmus von Golin/Li \cite[Fig. 6 und Kapitel 4]{papergolinli}, Kapitel IV, wobei die Logik des Left Shifts vereinfacht wurde:\\
\newline

Der Schritt 1 - Einlesung der Daten und der Schritt 2 – Häufigkeitsanalyse der Symbole im Text, aus Kapitel 1 und 2.2 bleibt gleich. Für kommende Berechnungen mit den Durchmessern der Perlen werden diese allerdings als Doubles gespeichert. 
\begin{enumerate}
  \item [3.] \textbf{Berechnung der Charakteristischen Wurzel $c$} \\
  Die Bestimmung der charakteristischen Wurzel $c$ ist ein wichtiger Teilschritt, der für die Aufteilung in Bins benötigt wird. Um $c$ numerisch zu berechnen, wird die Gleichung \ref{eq:4} gelöst, indem das Bisektionsverfahren verwendet wird. Das funktioniert, indem es schrittweise das Intervall $[0,10]$ halbiert und jeweils entscheidet, ob die Nullstelle der Funktion $f(c) = \sum 2^{-c \cdot c_m} - 1$ halbiert und jeweils entscheidet, ob die Nullstelle der Funktion $c<10$ da das das gewählte Startintervall ist.
  \item [4.] \textbf{Rekursive Codegenerierung}\\
  Für die Rekursion muss jeweils die linke Grenze (bezeichnet als $l$) und die rechte Grenze (bezeichnet als $r$) des betrachteten Intervalls bekannt sein. 
  \begin{enumerate}
    \item [0.] \textbf{Basisfall}\\
    Die Rekursion endet, wenn das betrachtete Teilintervall nur noch ein einziges Symbol enthält $(l == r)$. Der bis dahin akkumulierte Präfix $U$ (Pfad bis zum Bin) wird diesem Symbol als finales Codewort in der Codetabelle zugeordnet
    \item [1.] \textbf{Symbolzuweisung zu Initial-Bins}\\
    Jedes Symbol $i$ innerhalb der aktuellen Teilmenge ($l$ bis $r$) wird basierend auf seinem \\Wahrscheinlichkeitsmittelpunkt berechnet mit \ref{eq:5} zu dem Bin $m$ zugeordnet. Die Indizes der Symbole werden in \textit{initialBins} gesammelt. 
    \item [2.] \textbf{Left Shift (Stark vereinfacht)}\\
    Anstatt der komplexen Shifting Operation aus dem Originalalgorithmus implementiert der Code einen vereinfachten Left Shift. Er iteriert durch die \textit{initialBins}. Wenn ein Bin leer ist, wird versucht, ihn mit dem nächsten noch nicht zugewiesenen Symbol aus der sortierten Liste zu füllen. So wird die Idee, Lücken zu schließen, versucht zu bewahren. 
    \item [3.] \textbf{Right Shift}\\
    Der spezifische Fall, dass alle Symbole im ersten Bin landen (finalBins.size() $==$ 1), wird explizit überprüft. Wenn dies zutrifft und mehr als ein Symbol vorhanden ist, wird der Index des letzten Symbols aus dem ersten Bin entfernt und einem neu erstellten zweiten Bin hinzugefügt. 
    \item [4.] \textbf{Rekursiver Aufruf}\\
    Schließlich wird für jeden nicht leeren Bin ein rekursiver Aufruf gestartet. Der Codierungsbuchstabe des Bins wird dabei an den aktuellen Präfix $U$ angehängt, damit der Basisfall das Codewort festlegen kann. Der rekursive Aufruf erfolgt dann für die Teilmenge der Symbole in diesem Bin mit den angepassten Grenzen für das Betrachten des Intervalls. 
  \end{enumerate}
\end{enumerate}

Der Schritt 5 – Berechnung der Gesamtlänge des kodierten Texts und Schritt 6 – Ausgabe der Codetabelle, aus Kapitel 1 und 2.2 bleibt gleich. 

\subsubsection{Komplexitätsanalyse}
Nun wird die Komplexität der einzelnen Schritte des Algorithmus betrachtet, die Erklärungen für die unveränderten Teilschritten bleiben wie in den Kapitel 1 und 2.2 gleich. Dabei ist:
\begin{itemize}
  \item $L$ die Länge des Eingabetextes,
  \item $k$ die Anzahl der verschiedenen Zeichen (eindeutige Quellsymbole),
  \item $n$ die Anzahl der Kodierungsbuchstaben (Anzahl Perlenfarben).
\end{itemize}
\vspace{0.2cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Schritt} & \textbf{Laufzeit} & \textbf{Speicherbedarf} \\
\hline
Einlesen der Datei & $\mathcal{O}(L)$ & $\mathcal{O}(L)$ \\
Häufigkeitsanalyse & $\mathcal{O}(L)$ & $\mathcal{O}(k)$ \\
\textbf{Berechnung der Charakteristischen Wurzel} & $\mathcal{O}(n)$ & $\mathcal{O}(1)$ \\
\textbf{Rekursive Codegenerierung} & $\mathcal{O}(k \log k)$ & $\mathcal{O}(k \log k + k + n)$ \\
Berechnung der Gesamtlänge & $\mathcal{O}(k)$ & $\mathcal{O}(1)$ \\
Ausgabe Codetabelle (mit Sortierung) & $\mathcal{O}(k \log k)$ & $\mathcal{O}(k)$ \\
\hline
\textbf{Gesamt} & $\boldsymbol{\mathcal{O}(L + k \log k + n)}$ & $\boldsymbol{\mathcal{O}(L + k + n + k \log k)}$ \\
\hline
\end{tabular}
\caption{Laufzeit- und Speicherkomplexität Teilaufgabe B mit Strategie 2}
\end{table}

\begin{itemize}
  \item \textbf{Berechnung der Charakteristischen Wurzel $c$:} Bei jedem Aufruf der Funktion $f(c)$ muss die Summe $\sum_{i=1}^{n} 2^{-c \cdot c_{m}}$ berechnet werden. Dies braucht $t$ Potenzierungs- und Additionsoperationen. Die Laufzeit für eine einzelne Auswertung von $f(c)$ ist daher $\mathcal{O}(n)$. Da dies $100$-mal getan wird (Bisektionsverfahren 100-mal angewendet), gilt $\mathcal{O}(100 \cdot n)$ = $\mathcal{O}(n)$ 
  
  \item \textbf{Rekursive Codegenerierung:} Die rekursive Methode \textit{generateCodesGolinLi()} wird $\mathcal{O}(k)$ mal aufgerufen. Die zeitaufwendigste Operation in jedem Aufruf ist die Zuweisung von $k'$ Symbolen zu $n$ Bins, was $\mathcal{O}(n \cdot k')$ braucht. Die Gesamtlaufzeit hängt von der Rekursionstiefe und die Balance des Codebaums ab. Für diese Implementierung mit vereinfachtem Shifting ergibt sich daher geschätzt eine Laufzeit zwischen $\mathcal{O}(n \cdot k \log k)$ (balancierter Baum, aufgrund Aufteilung), da $n$ eine kleine Konstante ist: $\mathcal{O}(k \log k)$.

\end{itemize}

\vspace{1em}
Insgesamt ist die Laufzeit dominiert durch die rekurive Struktur des Algorithmus und die optionale Sortierung für die Ausgabe der Codetabelle. Der Speicherbedarf (hier geschätzt) ist etwas größer als bei Strategie 1, dennoch nicht bemerkenswertes. Die tatsächliche Speicherkomplexität hängt stark von der Struktur des generierten Codes ab und der Tiefe des Rekursionsstacks.
\newline
Wenn die Liste mit $n$ Kodierungsbuchstaben nicht sortiert wäre, wie in den Beispielen der BwInf Webseite, müsste diese noch sortiert werden und die Gesamtlaufzeit erhöht sich um $\mathcal{O}(n \log n)$. 
Daher erweist sich dieser Algorithmus auch bei größeren Texten als leistungsfähig und mit ungefähr derselben asymptoischen Laufzeit wie Strategie 1.

\subsubsection{Beispiele}
Hier die Ausgaben des Programmes zu den Beispielen auf der BwInf-Webseite. Mit Ausnahme der Datei \textit{schmuck5.txt} (für die ein offizieller Vergleichswert vorliegt), wurde die Konsolenausgabe auf die Gesamtlänge der kodierten Nachricht reduziert. Dies wurde gemacht für eine klarere Vergleichbarkeit der Ergebnisse zwischen den noch folgenden Algorithmen. \\Die gesamte Ausgabe ist hier \textbf{AusgabenBStrategie2.txt} \\ \newline \textbf{EDIT} \\ \newline zufinden. 
Die Ausgabe wird zu dem finalen (und besten) Programm ausführlicher gegeben. 


\begin{enumerate}
  \item \textbf{schmuck1.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 195 (Anzahl in Perlen) bzw. 19.5cm
\end{verbatim}
  \item \textbf{schmuck2.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 135 (Anzahl in Perlen) bzw. 13.5cm
\end{verbatim}
  \item \textbf{schmuck3.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 287 (Anzahl in Perlen) bzw. 28.7cm
\end{verbatim}
  \item \textbf{schmuck4.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 137 (Anzahl in Perlen) bzw. 13.7cm
\end{verbatim}
  \item \textbf{schmuck5.txt}
\begin{verbatim}
Charakteristische Wurzel c = 1.386461

Codetabelle (Golin/Li):
{
  '␣': 00 (Freq: 151, Länge: 2)
  'e': 010 (Freq: 110, Länge: 3)
  't': 011 (Freq: 71, Länge: 3)
  'i': 02 (Freq: 67, Länge: 3)
  'o': 100 (Freq: 64, Länge: 3)
  's': 101 (Freq: 61, Länge: 3)
  'a': 110 (Freq: 58, Länge: 3)
  'n': 111 (Freq: 56, Länge: 3)
  'r': 112 (Freq: 51, Länge: 4)
  'c': 12 (Freq: 44, Länge: 3)
  'd': 13 (Freq: 37, Länge: 4)
  'l': 200 (Freq: 33, Länge: 4)
  'm': 201 (Freq: 27, Länge: 4)
  'h': 210 (Freq: 26, Länge: 4)
  'u': 211 (Freq: 25, Länge: 4)
  'f': 22 (Freq: 20, Länge: 4)
  'p': 23 (Freq: 20, Länge: 5)
  'b': 30 (Freq: 16, Länge: 4)
  'y': 310 (Freq: 13, Länge: 5)
  'g': 311 (Freq: 11, Länge: 5)
  '.': 312 (Freq: 8, Länge: 6)
  ',': 32 (Freq: 5, Länge: 5)
  'q': 33 (Freq: 5, Länge: 6)
  'v': 400 (Freq: 5, Länge: 6)
  'x': 401 (Freq: 5, Länge: 6)
  'w': 410 (Freq: 4, Länge: 6)
  'T': 42 (Freq: 3, Länge: 6)
  'F': 411 (Freq: 3, Länge: 6)
  'A': 43 (Freq: 1, Länge: 7)
  'G': 44 (Freq: 1, Länge: 8)
  '2': 52 (Freq: 1, Länge: 7)
  '5': 53 (Freq: 1, Länge: 8)
  '9': 60 (Freq: 1, Länge: 7)
  'z': 61 (Freq: 1, Länge: 7)
  ';': 62 (Freq: 1, Länge: 8)
  'H': 500 (Freq: 1, Länge: 7)
  'S': 501 (Freq: 1, Länge: 7)
   '’': 502 (Freq: 1, Länge: 8)
  'j': 510 (Freq: 1, Länge: 7)
  '-': 511 (Freq: 1, Länge: 7)
  '1': 512 (Freq: 1, Länge: 8)
}

Gesamtlänge der Botschaft 3374 (Anzahl in Perlen) bzw. 337.4cm
\end{verbatim}

Dieses Ergebnis (3374) ist ca. $6.7\%$ schlechter als der Vergleichswert (3162) von der BwInf Webseite, also bereits eine großer Fortschritt im Verleich zu Strategie 1.
  \item \textbf{schmuck6.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 240 (Anzahl in Perlen) bzw. 24.0cm
\end{verbatim}
  \item \textbf{schmuck7.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 144964 (Anzahl in Perlen) bzw. 14496.4cm
\end{verbatim}
  \item \textbf{schmuck8.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 3433 (Anzahl in Perlen) bzw. 343.3cm
\end{verbatim}
  \item \textbf{schmuck9.txt}
\begin{verbatim}
  ...
Gesamtlänge der Botschaft 38516 (Anzahl in Perlen) bzw. 3851.6cm
\end{verbatim}
Dieses Ergebnis (38516) ist ca. $5.2\%$ schlechter als der Vergleichswert (36597) von der BwInf Webseite, daher eigentlich schon akzeptabel.
\end{enumerate}

\subsubsection{Quellcode}
\textbf{Wichtige Teile des Quellcodes:}\\

\textbf{Berechnung der Charakteristische Wurzel $c$}
\begin{lstlisting}
// 3. Find Characteristic root c for equation: sum(2^(-c*d_i)) = 1
private static double findCharacteristicRoot(double[] diameters) {
    // Define function f(c) = sum(2^(-c*d_i)) - 1
    Function<Double, Double> f = (c) -> {
        double sum = 0.0;
        for (double d : diameters) {
            sum += Math.pow(2.0, -c * d);
        }
        return sum - 1.0;
    };

    // Find c, numberic approach with bisection
    double low = 0.0;
    double high = 10.0; // Assumption: c not too big
    // Check limits
    if (f.apply(TOLERANCE) < 0)
        return Double.NaN; // No positiv root possible

    // I randomly chose 100 iterations, probably good enough
    for (int i = 0; i < 100; i++) {
        double mid = low + (high - low) / 2.0;
        double f_mid = f.apply(mid);

        if (Math.abs(f_mid) < TOLERANCE) {
            return mid;
        } else if (f_mid > 0) { // Root on the right
            low = mid;
        } else { // Root on the left
            high = mid;
        }
    }
    // Return best value
    return low + (high - low) / 2.0;
}
\end{lstlisting}

\textbf{Rekursive Codegenerierung}
\begin{lstlisting}
// 4. Recursive method for code generation with pseudocode from Golin/Li as basis
// (Fig.6 in paper)
// U is the current prefix code
private static void generateCodesGolinLi(List<Symbol> symbols, int l, int r,
        String U, Map<Character, String> codeTable,
        List<Perle> sortedDiameters, double cRoot) {

    // 0. Base case, only one symbol in [l, r]
    if (l == r) {
        Symbol sym = symbols.get(l);
        codeTable.put(sym.character, U);
        return;
    }
    if (l > r)
        return;

    // 1. Create initial bins Im*
    int k = sortedDiameters.size();
    // List of List with original indexes
    List<List<Integer>> initialBins = new ArrayList<>(k);
    for (int i = 0; i < k; i++) {
        initialBins.add(new ArrayList<>());
    }

    double P_start = symbols.get(l).cumulativeProbStart; // P_{l-1}
    double P_end = symbols.get(r).cumulativeProbEnd; // P_r
    // Total weight of this recursion step (w(v))
    double w = P_end - P_start;

    // No more weight to distribute
    if (w <= TOLERANCE)
        return;

    // L in paper
    double currentL = P_start;

    // Calculate edges Lm*, Rm* (relative limits inside of w) and assign symbols
    // Index m is Bin m+1 in Paper (0 to k-1 here)
    for (int m = 0; m < k; m++) {
        Perle diamInfo = sortedDiameters.get(m);
        double binWidth = w * Math.pow(2.0, -cRoot * diamInfo.cost);
        double currentR = currentL + binWidth;

        // Find symbols with middle point in [currentL, currentR)
        for (int i = l; i <= r; i++) {
            Symbol sym = symbols.get(i);
            // Centerpoint s_i = P_{i-1} + p_i / 2
            // Wahrscheinlichkeitsmittelpunkt, Heuristik
            double midPoint = sym.cumulativeProbStart + sym.probability / 2.0;

            // Check if fits to Bin m
            if (midPoint >= currentL - TOLERANCE && midPoint < currentR - TOLERANCE) {
                // Add original index to Bin m
                initialBins.get(m).add(i);
            }
        }
        // Update for next Bin
        currentL = currentR;
    }

    // 2. Very simplified version of left shift from paper
    // Just creates the final bin lists and skips empty bins
    List<List<Integer>> finalBins = new ArrayList<>();
    // Follow index of next available symbol
    int firstItemIndex = l;
    for (int m = 0; m < k; m++) {
        List<Integer> currentBinContent = initialBins.get(m);
        if (currentBinContent.isEmpty()) {
            // Bin is empty: Geht next availalbe symbol
            if (firstItemIndex <= r) {
                // (In paper: find bin where the symbol was originally -> very complex, I did
                // not manage to implement that correctly :(, so here just add next element to
                // bin to fill it)
                finalBins.add(Collections.singletonList(firstItemIndex));
                firstItemIndex++;
            } else {
                // No more elements to fill bin -> bin stays empty (ignored, very sad I know)
            }
        } else {
            List<Integer> effectiveBinContent = new ArrayList<>();
            for (int originalIndex : currentBinContent) {
                // Just add if not already used by shifting
                if (originalIndex >= firstItemIndex) {
                    effectiveBinContent.add(originalIndex);
                }
            }
            // If bin empty because of skipping, but there are still elements, then just use
            // next
            if (effectiveBinContent.isEmpty() && firstItemIndex <= r) {
                finalBins.add(Collections.singletonList(firstItemIndex));
                firstItemIndex++;
            } else if (!effectiveBinContent.isEmpty()) {
                finalBins.add(effectiveBinContent);
                // Set index for next element after last of this bin
                firstItemIndex = effectiveBinContent.get(effectiveBinContent.size() - 1) + 1;
            }
            // Otherwise bin stays empty
        }
    }
    // Check if all elements were assigned
    if (firstItemIndex <= r) {
        // Elements are still there, that are not inside a bin -> problem somewhere
        // Add to last non empty bin
        System.out.println(
                "Irgendwas broken, eyyy ich kann nicht mehr, Elemente [" + firstItemIndex + " ... " + r + "] nicht zugeordnet");
        if (!finalBins.isEmpty()) {
            List<Integer> lastBin = finalBins.get(finalBins.size() - 1);
            for (int i = firstItemIndex; i <= r; i++) {
                lastBin.add(i);
            }
        } else {
            System.out.println("FATAL");
            // Fill first bin, emergency lol
            List<Integer> onlyBin = new ArrayList<>();
            for (int i = l; i <= r; i++)
                onlyBin.add(i);
            finalBins.add(onlyBin);
        }
    }

    // 3. Right shift
    // in paper: if all in bin 1 then move last element to bin 2
    // Here checking this
    if (finalBins.size() == 1 && finalBins.get(0).size() == (r - l + 1) && k > 1) {
        List<Integer> firstBin = finalBins.get(0);
        // Only makes sense if more than 1 element in bin
        if (firstBin.size() > 1) {
            // Remove last index
            int lastElementIndex = firstBin.remove(firstBin.size() - 1);
            List<Integer> secondBin = new ArrayList<>();
            secondBin.add(lastElementIndex);
            // Only add if space
            if (finalBins.size() < k) {
                finalBins.add(secondBin);
            } else {
                System.out.println("No space in second bin");
                firstBin.add(lastElementIndex); // Revert
            }
        }
    }

    // 4. Recursion for every non empty final bin
    for (int m = 0; m < finalBins.size(); m++) {
        List<Integer> currentBinIndices = finalBins.get(m);
        if (!currentBinIndices.isEmpty()) {
            // First index in bin
            int bin_l = currentBinIndices.get(0);
            // Last index
            int bin_r = currentBinIndices.get(currentBinIndices.size() - 1);
            // Find color / index for this bin
            // in paper quite complex so here the m-th non empty bin gets the m-th cheapest
            // color
            int colorIndex = sortedDiameters.get(m).digit;

            // Recursive call
            generateCodesGolinLi(symbols, bin_l, bin_r, U + colorIndex, codeTable, sortedDiameters, cRoot);
        }
    }
}
\end{lstlisting}

\subsection{Strategie 3: Dynamische Programmierung Golin/Rote}
\subsubsection{Kernidee}
Der in Strategie 2 umgesetzte Algorithmus bringt zufriedenstellende Lösungen für dieses Teilproblem in einer sehr skalierbaren Laufzeit. Dennoch sind die Beispielängen der BwInf-Webseite besser als die berechneten Werte dieses Programms. Daher wird vermutet, dass die Werte die Gesamtlänge einer optimal kodierten Nachricht sind. Um diese Vermutung zu überprüfen, wird im Folgenden ein anspruchsvoller dynamischer Programmieransatz implementiert, der von Golin und Rote in \cite{papergolinrote} vorgestellt wird. Der Algorithmus garantiert eine optimale Kodierung von $k$ Quellsymbolen in einer Laufzeit von $\mathcal{O}(k^{C+2})$ , wenn die Kosten $C$ der Kodierungsbuchstaben eine ganze positive Zahl ist. Dieser Ansatz wurde zuletzt gewählt, da die Eingabebeispiele große Werte von $k$ haben (insbesondere \textit{schmuck8.txt} und \textit{schmuck9.txt}, wegen den vielen verschiedenen (chinesischen/japanischen) Zeichen) und $C$ bis zu 6 wird, was die Laufzeit stark beeinflusst, aber noch im Rahmen des Möglichen bleibt. Der Ansatz, das Problem in ein ILP umzuformen wurde versucht, aber aufgrund exponentieller Laufzeiten verworfen. \\
\newline
Aus Kapitel 1 und 2.1 ist klar geworden, dass der einfache greedy Ansatz von Huffman für ungleiche Buchstabenkosten nicht mehr garantiert zum optimalen Codebaum führt, aus dem die Codes erstellt werden können. Das Ziel des Folgenden beschriebenen Ansatzes ist es daher (zumindest vom Konzept her), die Konstruktion eines solchen optimalen Codebaums. 
\\\newline
Die Kernidee dieses Algorithmus ist es, das Problem der optimalen Codebaum-Konstruktion in ein Kürzeste-Wege-Problem auf einen \textbf{Zustandsgraphen} zu überführen. Anstatt wie Huffman den Baum von den Blättern zur Wurzel (Bottom-Up) aufzubauen, verfolgt dieser Ansatz eine Top-Down-Strategie mit dynamischer Programmierung (DP). DP ist eine Methode, bei dem ein Problem in Teilprobleme zerlegt wird, deren Lösungen gespeichert werden, um sie zur Berechnung der Gesamtlösung wiederzuverwenden.
\\\newline
Der Algorithmus erstellt gedanklich einen gewichteten, gerichteten, azyklischen Graphen. Jeder \textbf{Knoten} in diesem Graphen steht für einen eindeutigen \textbf{Zustand} während des schrittweisen Aufbaus des Codebaums. (Wichtig: Graph und Baum sind separat zu betrachten. Der Graph stellt Möglichkeiten dar, \textbf{wie} man den Codebaum aufbauen kann). Dieser Zustand wird als "\textbf{Signatur}" $S = (m; l_1, l_2, ..., l_C)$ bezeichnet. Diese Signatur fasst alle wichtigen Informationen über den aktuellen Baufortschritt des Baums zusammen (wenn es im Folgenden um den Zustandsgraphen geht, werden Signatur und Knoten und Zustand als Synonyme verwendet):
\begin{itemize}
  \item $m$: Gibt an, wie viele der $k$ Quellsymbole bereits einen finalen Platz im entstehenden Baum erhalten haben (d.h., ihr Pfad endet an einem Blatt). 
  \item $(m; l_1, l_2, ..., l_C)$:     Beschreiben die Anzahl der aktuell "offenen" Astenden (potenzielle Blätter) auf den nächsten $C$ relevanten Tiefen-Ebenen des Baumes. Diese bilden die "Front", an der der Baum weiter ausgebaut werden kann. 
\end{itemize}

Eine gerichtete Kante von einer Signatur $S$ zu einer Signatur $S'$ im Graphen steht für einen \textbf{Expansions- schritt} beim Aufbau des Baums. Dieser Schritt entscheidet, was mit der aktuellen "Wachstumsfront" des Baumes, wo sich $l_1$ offene Endknoten befinden, geschieht. Er entscheidet, wie diese offene Endknoten behandelt werden. Dies wird durch eine Variable $q$ quantifiziert, was die Anzahl dieser $l_1$ offenen Endknoten, die in diesem Schritt ausgewählt werden, um weiter zu expandieren, ist. Das bedeutet, diese $q$ Knoten werden zu inneren Knoten des Baumes und erhalten neue Kinder entsprechend den Buchstaben des Zielalphabets und deren Kosten. Die verbleibenden $(l_1 - q)$ Knoten dieser Gruppe werden in diesem Schritt zu finalen Blättern, wo die Pfade dann enden.  
\\
Das Gewicht der Kanten zwischen den Signaturen in dem Graph, gibt die zusätzlichen Kosten an, die durch diesen Expansionsschritt entstehen. Da sich durch den Expansionsschritt die Pfadlänge zu diesen noch offenen Enden erhöht, steigt ihr Beitrag zu den Gesamtkosten. Die Kantenkosten sind daher die Summe der Wahrscheinlichkeiten dieser "wartenden" Symbole.
\\
Ein wichtiger Aspekt (für die Effizienz, damit $\mathcal{O}(k^{C+2})$ erreicht wird, und nicht $\mathcal{O}(k(n-1)^{C+2})$) des Algorithmus, ist die \textbf{Reduktion der Signaturen}. Wenn ein Expansionsschritt zu einer Signatur führt, die mehr als die benötigten $k$ Blätter hätte, werden die überzähligen, tiefsten Blätter entfernt ("reduce"-Operation). Dadurch wird sichergestellt, dass nur die Signaturen betrachtet werden, die maximal $k$ Blätter kodieren, was die Größe des Zustandsgraphen eingrenzt. 
\\
Ein Pfad von der Signatur $S_0$ (repräsentiert die leere Wurzel der Baums) zum Zielzustand (alle $k$ Symbole sind im Baum platziert, keine offenen Enden mehr) entspricht der gesamten Sequenz für den Bau von einen präfixfreien Codebaum. Der kürzeste Pfad in diesem Zustandsgraphen entspricht der Sequenz des Baus des Codebaums mit den minimal möglichen durchschnittlichen Gesamtkosten. Die Länge (Summe der Kantengewichte) dieses kürzesten Pfades ist dieser optimale Kostenwert. 
\\
\newline
Da der Graph keine Zyklen enthält, kann der kürzeste Pfad effizient mit DP gefunden werden (hat Ähnlichkeiten zu Dijkstra mit der PriorityQueue). Der Algorithmus durchläuft die Zustände (Signaturen) so, dass sicherstellt, dass ein Zustand erst verarbeitet wird, wenn die optimalen Kosten dorthin bekannt sind (topologische Reihenfolge). Für jeden erreichten Zustand $S$ werden alle möglichen nächsten Zustände $S'$ (durch Expansion mit $q$) betrachtet. Die Kosten, um $S'$ über $S$ zu erreichen, werden mit den bisher besten bekannten Kosten für $S'$ verglichen und gegebenenfalls aktualisiert. Dabei wird gespeichert, von welchem Vorgänger $S$ und mit welcher Entscheidung $q$ der beste Weg nach $S'$ führte. Das bedeutet, zu jedem Zustand $S'$ wird gespeichert, wie man zu $S'$ gelangt ist.  
\\
Es ist anzumerken, dass der Zustandsgraph nicht explizit komplett aufgebaut und dann durchsucht wird. Dies wäre wegen der potenziell riesigen Anzahl möglicher Signaturen für größere Eingaben sehr ineffizient. Stattdessen wird der relevante Teil des Graphen implizit und schrittweise während der Ausführung des DP aufgebaut und gleichzeitig ausgewertet. Mit diesem Prozess wird der kürzeste Pfad gefunden und gespeichert, sowie seine Länge. Der gerade beschriebene Algorithmus zum Erstellen des Zustandsgraphen und das Finden des kürzesten Pfads wird in \cite{papergolinrote} vorgestellt. Das Finden der Codetabelle wird allerdings nicht mehr explizit ausgeführt und im Folgenden wird dieser Prozess beschrieben:
\\\newline
Sobald die minimalen Kosten zum Gelangen zu dem Zielzustand berechnet sind, wird der optimale Weg rekonstruiert. Durch \textbf{Backtracking} (Zurückverfolgen der gespeicherten optimalen Vorgänger vom Ziel zum Startknoten) erhält man den kürzesten Pfad. Dieser Pfad repräsentiert die \textbf{Sequenz der optimalen Expansionsentscheidungen ($q$-Werte)}, die zur Konstruktion des optimalen Codebaums führen.
\\
Diese Sequenz dient als Bauplan für diesen Baum. Anstatt nun einen expliziten Baum im Speicher aufzubauen, kann der optimale Codebaum simuliert werden, indem dieser Bauplan Schritt für Schritt abgearbeitet wird. Dabei werden die aktiven Codewort-Präfixe und ihre Tiefen verwaltet und die Codewörter werden direkt generiert.
\\
Obwohl sich die Konstruktion der optimalen Baumstruktur wegen der ungleichen Buchstabenkosten stark vom Standard-Huffman unterscheidet, bleibt das Prinzip der Zuweisung von Symbolen zu Blättern dasselbe: Das Quellsymbol mit der höchsten Wahrscheinlichkeit wird dem Blatt mit der geringsten Tiefe im resultierenden optimalen Baum zugeordnet, das zweitwahrscheinlichste dem Blatt mit der zweitgeringsten Tiefe usw. Die finalen Codewörter ergeben sich dann wie üblich aus den Pfaden von der Wurzel zu diesen zugewiesenen Blättern. In 2.4.2 (Umsetzung) wird genauer auf die verwendete Methode des Erstellens der Codetabelle eingegangen
\\\newline
Zusammengefasst funktioniert der Algorithmus aus \cite{papergolinrote}, da mit der dynamischer Programmierung gezielt alle sinnvollen Möglichkeiten durchgegangen werden, wie man den Codebaum aufbauen kann. Die Signatur speichert dabei alle wichtigen Infos, die später gebraucht werden, um die bestmöglichen Entscheid- ungen des Baumaufbaus zu treffen. Da die Kosten im Zustandsgraph genau den tatsächlichen Aufbaukosten des Codebaums entsprechen, garantiert der kürzeste Weg im Graph, dass die günstigste Bauweise des Codes gefunden wird.


\subsubsection{Umsetzung}
Die Implementierung des DPs für den Graphen basiert auf der in \cite[Figure 6]{papergolinrote} dargestellten Struktur des Algorithmus. Im Folgenden wird der gesamte Prozess zur Lösung der Teilaufgabe B gezeigt, wobei der Fokus auf den Schritten liegt, die zur Berechnung der optimalen Kosten führen und aufzeigen, wie daraus die finale Codetabelle erstellt wird. \\
Wrapperklassen, die die Ergebnisse von einzelnen Schritten speichern, um sie zu den nächsten Schritten zu Verfügung zu stellen, werden nicht extra explizit angesprochen. Eine wichtige häufig verwendete Klasse ist allerdings die \textbf{Signature} Klasse, die eine Signatur im Graphen darstellt und die in 2.4.1 entsprechend beschriebene Attribute speichert. 
\\\newline
Der Code ist wie folgt strukturiert:
\begin{enumerate}
  \item \textbf{Vorverarbeitung und Initialisierung}
  \begin{enumerate}
    \item \textbf{Einlesen der Daten und Frequenzanalyse}\\
    Diese zwei Teilschritte sind identisch zu der implementierten Version in den vorherigen Strategien. Allerdings wird zusätzlich die eingelesenen Zeichen bei der Frequenzanalyse bereits hier nach ihrer Häufigkeit sortiert, da dies für kommende Schritte nötig ist. 
    \item \textbf{Berechnung der kumulativen Frequenzen}\\
    Um die Kosten für die Kanten bei der Erstellung des Zustandsgraphen im Graphen effizient zu berechnen, wird ein Array erstellt. Das $i$-te Element ist dabei die Summe der Häufigkeiten der Symbole von Index $i$ bis $k-1$. Dies erlaubt die Berechnung der Kantenkosten (Summe der Wahrscheinlichkeiten der noch nicht platzierten Symbole) in $\mathcal{O}(1)$. 
    \item \textbf{Berechnung des charakteristischen Vektors}\\
    Aus der Liste der Kodierungsbuchstabenkosten wird der charkteristische Verktor $d$ berechnet. Dieser Vektor $d=(d_1, ..., d_C)$ ist eine Zusammenfassung der Kostenstruktur des Zielalphabets, denn er gibt an, wie viele Kodierungsbuchstaben ($d_j$) genau die Kosten $j$ haben. Er wird im Algorithmus gebraucht, um bei einem Expansionsschritt (also wenn $q$ Knoten erweitert werden) zu berechnen, wie viele neue "offene Enden" (potenzielle Blätter) auf welchen relativen Tiefen $(+1, +2, ..., +C)$ entstehen und wie sich dadurch die Signatur für den nächsten Zustand verändert vgl. \cite[Lemma 3]{papergolinrote}.
  \end{enumerate}
  \item \textbf{DP zur Kostenminimierung}
  \begin{enumerate}
    \item \textbf{Erstellung der Variablen für die DP}\\
    Für den kommenden Teil mit der Dynamischen Programmierung müssen wichtige Datenstrukturen erstellt werden, um die Ergebnisse vom DP zu speichern: 
    \begin{itemize}
      \item HashMap \textit{opt}: speichert für jede bisher erreichte Signatur, was die bisher gefundenen minimalen Kosten sind ($OPT[S]$ im Paper).
      \item HashMap \textit{pred}: speichert, von welcher Vorgänger-Signatur und mit welcher Expansionsentscheidung $q$ sie optimal erreicht wurde. Dies ist essenziell für das spätere Backtracking. 
      \item Es wird (wie bei Dijkstra) ein Start $S_0$ und Zielknoten finalState für den Graphen definiert ($S_0$: Startsignatur, finalState: Zielsignatur mit $(k; 0, ..., 0)$), damit klar ist, wann mit der Suche nach dem kürzesten Pfad aufgehört werden kann. 
      \item Eine PriorityQueue $pq$ wird erstellt und $S_0$ wird hinzugefügt. Sie steuert die Exploration des Zustandsgraphen in einer ähnlichen Weise wie Dijkstra, indem Zustände mit den aktuell geringsten Kosten zuerst verarbeitet werden, was die topologische Reihenfolge sicherstellt. 
    \end{itemize}
  \item \textbf{Hauptschleife für DP}\\
  In diesem Abschnitt wird der im Paper beschriebene DP-Algorithmus zur impliziten Konstruktion und Exploration des Zustandgraphen implementiert: \\
  \newline
  Die Hauptschleife wird definiert: Solange es Zustände in der $pq$ gibt, wird der Zustand \textit{current} mit den geringsten Kosten \textit{currentCost} entnommen. Es wird überprüft, ob dieser Zustand bereits über einen kürzeren Pfad erreicht wurde oder ob das Ziel bereits mit geringeren Kosten gefunden wurde. Außerdem wird überprüft, ob der Zielknoten \textit{finalState} erreicht wurde. 
  \newline
  Innerhalb dieser Schleife wird auch der in 2.4.1 beschriebene Expansionsschritt durchgeführt, wo für die aktuelle Signatur alle möglichen nächsten Zustände generiert werden:
  \begin{enumerate}
    \item Die Anzahl der offenen Enden an der Front ($l_1$) wird aus der Signatur geholt 
    \item Die Kosten für den Expansionsschritt werden mit dem Array aus Schritt 1b berechnet 
    \item Eine Schleife iteriert über alle möglichen Expansionsentscheidungen $q$ (von 0 bis $l_1$)
    \item Für jedes $q$ wird die nächste Signatur (vor der Reduktion der Singaturen) berechnet  
    \item Anschließend wird der in 2.4.1 beschriebene Prozess von der Reduktion der Signaturen angewendet 
    \item Wenn nun der Pfad zu dieser neuen Signatur besser ist, werden \textit{pred} und \textit{opt} aktualisiert  
  \end{enumerate}

  \end{enumerate}
  \item \textbf{Ergebnissicherung von DP}\\
  Nach der DP-Schleife wird überprüft, ob tatsächlich ein Endzustand erreicht wurde und die finalen minimalen Kosten (Länge des kürzesten Pfads im Graph) abgespeichert. 
  \item \textbf{Rekonstruktion der Blattiefen}\\
  Nun wird der erste Teil der Erstellung der Codetabelle durchgeführt. Ein intuitiver Ansatz wäre, mittels Backtracking den optimalen Pfad zurückzuverfolgen und daraus den optimalen Codebaum zu erstellen. Allerdings bin ich bei der Recherche für diese Aufgabe auf den canonical Huffman Code gestoßen, der die Codes generiert, ohne dabei den Baum explizit zu bauen. Das fand ich interessant und die kleine (also… in meiner Vorstellung kleine) Änderung wollte ich implementieren. Spoiler: Die geplante Stunde wurde zu 20, meine To-do-Liste hat gelacht, und mein Schlafrhythmus wurde (mal wieder) dabei rekursiv aufgelöst – leider ohne Memoization. Ich habe leider viel zu spät mit der Bearbeitung dieser Aufgabe angefangen, aber naja passiert... Daher wird im Folgenden der erste Teil der Erstellung der Codetabelle, basierend auf den canonical Huffman Code, ohne die explizite Erstellung des Codebaums, implementiert. Dies wird gemacht, indem der optimale Pfad genommen wird, um zu ermitteln, wie viele der $k$ Blätter im optimalen Baum auf welcher spezifischen Tiefe landen müssen. Mit dem aus Schritt 2 berechneten Pfad wird der optimale Bauprozess des Codebaums "nachgespielt", wobei nur gesichert wird, wann die Blätter finalisiert werden und auf welcher Tiefe. Denn um die finale Codetabelle zu erstellen, wird nicht die volle Baumstruktur benötigt, sondern nur die \textbf{Verteilung der Blatttiefen}.  
  \begin{enumerate}
    \item Zunächst wird mit der \textit{pred} Map der Pfad von der Zielsignatur zurück zur Startsignatur $S_0$ verfolgt. Die dabei gesammelten Informationen zur Erstellung des Baums, wie die $q$ Werte, werden in einer Liste path gespeichert und anschließend umgedreht, um die optimale Sequenz von $S_0$ zum Ziel zu erhalten. 
    \item Eine HashMap \textit{leavesFinishedAtDepth} wird angelegt, in der das Endresultat gespeichert wird. Diese soll die exakte Anzahl der Blätter, die im optimalen Codebaum auf jeder Tiefe enden müssen, halten. Zusätzlich werden direkt die Blätter, die nach dem Start bei $S_0$ existieren, unter der Berücksichtigung der initialen Reduktion, ermittelt. \\
    Um die Vorwärtssimulation der Erstellung des Baums zu implementieren, wird durch die optimale $q$-Sequenz (\textit{path}) iteriert. Innerhalb dieser Schleife wird bestimmt, wie viele der Blätter an der aktuellen Front expandiert und wie viele finalisiert werden. Anschließend wird die Reduktionslogik simuliert, indem berechnet wird, wie viele Blätter insgesamt existieren würden und wie viele davon entfernt werden müssen, um $k$ nicht zu überschreiten. Dann wird die Anzahl der Blätter, die bei der aktuellen Simulationstiefe \textit{simDepth} tatsächlich finalisiert werden, berechnet und in die \textit{leavesFinishedAtDepth} Map hinzugefügt. Schließlich wird überprüft, ob die berechneten Werte tatsächlich stimmen und \textit{leavesFinishedAtDepth} wird zurückgegeben.  
  \end{enumerate}
  \item \textbf{Generierung der Codetabelle}\\
  Um nun die Codetabelle zu erstellen, werden die aus Schritt 1a. (Frequenzanalyse) und Schritt 4. (Blatttiefen) berechneten Werte verwendet. Diese Methode funktioniert, indem man diese Anforderungen (die berechnenten Werte) bekommt und daraus eine konkrete, präfixfreie Codetabelle erstellt, die diese Anforderungen erfüllt. Dabei ist sie unabhängig vom Golin-Rote-Algorithmus und betrachtet nur das Teilproblem: "Finde eine Menge von $k$ präfixfreien Codes, sodass diese Codes die vorgegebene Tiefe besitzen, und ordne diese den Symbolen $P_1$ bis $P_k$ in der gegebenen Reihenfolge zu." Dies ist ein Standardproblem. Wenn man die Längen (oder hier äquivalent: die Kosten/Tiefen) der Codewörter eines optimalen Codes kennt, gibt es Algorithmen, um einen tatsächlichen präfixfreien Code mit genau diesen Längen zu konstruieren. Das bekannteste Beispiel dafür ist die Konstruktion von \textbf{Canonical Huffman Codes}. Das Folgende implementiert eine Variante davon, der einen systematischen, greedy Generierungs- und Zuweisungsansatz verfolgt: 
  \\\newline
  Zunächst wird eine Liste von $k$ Targets (TargetInfo Objekte, bzw. auch Ziele genannt) erstellt. In dieser Liste wird für jedes der $k$ Symbolen, seine zugehörige optimale Tiefe (analog zu seinen Kosten) gespeichert. Diese Liste wird anschließend nach Tiefe sortiert, damit Codewörter für geringere Tiefen bevorzugt zugewiesen werden. Außerdem wird eine Kandidaten Liste \textit{candidatePool} erzeugt, die generierte, aber noch nicht verwendete, Codewort-Kandidaten speichert. \\
  Dann erzeugt ein Generator (eine Deque) systematisch die Codewort-Kandidaten $("0", "1", ..., "00",\\ "01", ...)$. Es wird über die Targets, die jeweils ein Symbol $s$ und seine benötigte Tiefe $d$ speichert, iteriert, beginnend mit dem mit den geringsten Kosten. Zuerst wird im \textit{candiatePool} nach einem passenden Kandidaten für $s$ gesucht. Wenn dieser präfixfrei zu bereits vergebenen Codes ist, wird dieser verwendet. Falls kein passender Code im Pool war, werden neue Kandidaten aus dem Generator genommen. Dabei wird jeder Kandidat auf Kosten und Präfixfreiheit überprüft: 
  \begin{itemize}
    \item Kosten passt und präfixfrei: Code wird zugewiesen 
    \item Kosten passt, aber nicht präfixfrei: Kandidat wird (zurück) in den Pool gelegt 
    \item Kosten passt nicht: Kandidat wird in den Pool gelegt 
  \end{itemize}
  Dieser Generierungs- und Prüfprozess wird fortgesetzt, bis ein passender, präfixfreier Code für das aktuelle Target gefunden wird oder ein Generierungslimit erreicht ist. 
  \\\newline
  Nachdem alle $k$ Ziele abgearbeitet wurde, wurde die vollständige, präfixfreie Codetabelle gefunden. 
  \item \textbf{Ausgabe der Codetabelle} \\
  Analog zu den bisherigen Strategien
  
\end{enumerate}

\subsubsection{Komplexitätsanalyse}
Für das Verständnis ist es empfehlenswert, den Code nebenbei offen zu haben, um die Laufzeiten nachzuvollziehen. Für die Komplexitätsanalyse werden zur besseren Übersicht die verwendeten Variablen im Folgenden nochmals erläutert:
\begin{itemize}
  \item $L$: Gesamtlänge des Eingabetextes
  \item $k$: Anzahl der einzigartigen Quellsymbole (im Paper oft als $n$ bezeichnet)
  \item $n$: Größe des Zielalphabets (Anzahl der verschiedenen Kostenwerte)
  \item $C$: Maximale Kosten eines Zeichens des Zielalphabets
  \item $V$: Anzahl der Knoten (Signaturen) im impliziten Zustandsgraphen
  \item $E$: Anzahl der Kanten im impliziten Zustandsgraphen
\end{itemize}

\vspace{0.2cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Schritt} & \textbf{Laufzeit} & \textbf{Speicherbedarf} \\
\hline
\textbf{Vorverarbeitung} & $\mathcal{O}(L + k \log k)$ & $\mathcal{O}(k)$ \\
\textbf{DP zur Kostenminimierung} & $\mathcal{O}(k^{C+2}\cdot C)$ & $\mathcal{O}(C \cdot k^{C+1})$ \\
\textbf{Ergebnissicherung von DP} & $\mathcal{O}(k^{C+1}\cdot C)$ & $\mathcal{O}(1)$ \\
\textbf{Rekonstruktion der Blatttiefen} & $\mathcal{O}(L_{path} \cdot k \log k)$ & $\mathcal{O}(L_{path} \cdot C)$ \\
\textbf{Generierung der Codetabelle} & $\mathcal{O}(>k\cdot L_{max})$ & $\mathcal{O}(>k\cdot L_{\text{max}})$ \\
Ausgabe Codetabelle & $\mathcal{O}(k \log k)$ & $\mathcal{O}(k)$ \\
\hline
\textbf{Gesamt} & $\boldsymbol{\mathcal{O}(C\cdot k^{C+2})}$ & $\boldsymbol{\mathcal{O}(C\cdot k^{C+1})}$ \\
\hline
\end{tabular}
\caption{Laufzeit- und Speicherkomplexität Teilaufgabe B mit Strategie 3}
\end{table}

\begin{itemize}
  \item \textbf{Vorverarbeitung}:
  \begin{itemize}
    \item \textbf{Laufzeit}:
      Häufigkeitsanalyse iteriert durch den Text $\mathcal{O}(L)$. Sortieren der $k$ Symbole liegt in $\mathcal{O}(k \log k)$; Gesamt: $\mathcal{O}(L + k \log k)$
    \item \textbf{Speicher}: \textit{frequencyMap, sortedSymbols, freqCounts, cumFreq} sind jeweils in $\mathcal{O}(k)$, der charakteristische Vektor ist ein Array der Größe $C \mathcal{O}(C)$; Gesamt: Dominiert von $\mathcal{O}(k)$.
  \end{itemize}
  \item \textbf{DP zur Kostenminimierung}:
  \begin{itemize}
    \item \textbf{Laufzeit}: Schritt 2a in $\mathcal{O}(C)$. Die Hauptschleife von Schritt 2b. läuft im schlimmsten Fall so oft, wie Kanten im Graphen (ähnlich wie Dijkstra), was $\mathcal{O}(E)$ ist. Die kleinste Signatur aus der PriorityQueue zu holen, ist in $\mathcal{O}(\log |PQ|)$, wo $|PQ|$ die Größe der Queue ist. Da sie im schlimmsten Fall so groß wird wie $V$: $\mathcal{O}(\log V) = \mathcal{O}(\log (k^{C+1})) = \mathcal{O}(C\log k)$.\\
    Das Überprüfen, ob der Zielknoten erreicht wurde, ist in $\mathcal{O}(C)$, da $m$ und die Liste der Länge $C$ verglichen werden muss. Die Kostenberechnung der Kanten ist konstant wegen \textit{cumFreq} und die Schleife für die Expansionsentscheidungen läuft bis zu $\mathcal{O}(k)$ mal, da sie $l_1 + 1$  mal läuft und $l_1 \le k$. In dieser Schleife ist alles in $\mathcal{O}(C)$ bis auf das Hinzufügen in die PriorityQueue das in $\mathcal{O}(\log |PQ|) = \mathcal{O}(C\log k)$, um das eingefügte Element an die richtige Position zu bringen, da sie als Min-Heap implementiert ist. \\
    Die Kosten pro Iteration der $q$ Schleife (Expansionsentscheidungen) ist somit bei $\mathcal{O}(C+ C\log k)=\mathcal{O}(C\log k)$.\\
    Die Hauptschleife von 2b hat somit Kosten von $\mathcal{O}(C\log k) \cdot \mathcal{O}(k \cdot C \log k)$ pro Iteration.\\
    Gesamtkosten: Normalerweise wäre mit der PriorityQueue die Laufzeit bei ca. $\mathcal{O}(E \log V) = \mathcal{O}(C\cdot k^{C+2}\cdot \log k)$ (siehe Dijkstra). Aber \cite[Richtung Ende Kapitel 4]{papergolinrote} argumentiert, dass die Laufzeit in $\mathcal{O}(C\cdot k^{C+2})$ ist, wegen der Struktur des Problems und auch wegen der (impliziten) topologischen Verarbeitsungsreihenfolge. $C$ wird allerdings oft weggelassen, da diese in der Realität oft nur eine sehr kleine Konstante ist.
    \item \textbf{Speicher}:
    Der Speicher ist hier dominiert von den HashMaps \textit{opt, pred} und der PriorityQueue \textit{pq}. Die Maps sind in $\mathcal{O}(k^{C+1})$, da die erreichten Zustände als Schlüssel gespeichert werden. Die Anzahl der Zustände $V'$ ist $\mathcal{O}(k^{C+1})$, wo $V'$ die Anzahl der Knoten im reduzierten Graphen ist und jede Signatur speichert eine Liste der Länge $C$. Die PriorityQueue kann im schlimmsten Fall einen sehr großen Teil der Zustände haben, bevor sie verarbeitet werden. Sie ist in der selben Komplexität wie die Maps, da sie bis zu $\mathcal{O}(V')$ groß werden kann.
  \end{itemize}
  \item \textbf{Ergebnissicherung von DP}:
  \begin{itemize}
    \item \textbf{Laufzeit}:
    Um zu überprüfen, ob ein Zielknoten erreicht wurde (auch anderes Objekt, mit denselben Signatur), muss im schlimmsten Fall über $V$ Elemente iteriert werden, die jeweils $\mathcal{O}(C)$ brauchen, um überprüft zu werden. Gesamt daher in $\mathcal{O}(k^{C+1} \cdot C)$.
    \item \textbf{Speicher}: Konstant
  \end{itemize}
  \item \textbf{Rekonstruktion der Blatttiefen}:
  \begin{itemize}
    \item \textbf{Laufzeit}:
    Für das Backtracking wird $\mathcal{O}(L_{path})$ gebraucht, mit $L_{path}$ als Länge des optimalen Pfads. Für die Simulation für das Erstellen des Baums, läuft die Schleife $L_{path}$ mal. Innerhalb dieser Schleife dominiert das Sortieren von \textit{activeLeaves.Keyset()} mit einer Laufzeit von $\mathcal{O}(k \log k)$. Gesamt: Im worst Case also ca. $\mathcal{O}(L_{path}\cdot k \log k)$.
    \item \textbf{Speicher}:
    Hier dominiert die Liste \textit{path} mit $\mathcal{O}(L_{path} \cdot C)$.
  \end{itemize}
  \item \textbf{Generierung der Codetabelle}:
  \begin{itemize}
    \item \textbf{Laufzeit}:
    Um zunächst die Targets (Ziele) zu erstellen, iteriert die Schleife über \textit{depthsCounts} und $k$ Symbole. Geschätzt benötigt das $\mathcal{O}(k+ k\log k)$, da die Tiefen danach noch sortiert werden. Die Hauptschleife iteriert über $k$ Targets und die Pool Prüfung iteriert über maximal $k$ zugewiesene Codes. Dort braucht der Präfix Check, wenn $L_{max}$ die maximale Codewortlänge ist, $\mathcal{O}(L_{max})$. Also braucht jeder Kandidat $\mathcal{O}(k\cdot L_{max})$. \textbf{Insgesamt hängt die Laufzeit stark davon ab, wie schnell passende, präfixfreie Codes gefunden werden}. Im best Case wenn sie schnell gefunden werden $\mathcal{O}(k\cdot L_{max})$. Der worst Case kann sehr groß sein, was aber nicht direkt einfach von $k$ oder $C$ abhängt.
    \item \textbf{Speicher}:
    Der Speicher für \textit{codeMap, assignedCodesList} ist bei $\mathcal{O}(k\cdot L_{max})$. Da der Generator theoretisch sehr groß werden kann, ist der Speicher für diese Methode schwer abzuschätzen, sollte aber in der Praxis noch akzeptabel sein.
  \end{itemize}

\end{itemize}
\subsubsection{Beispiele}
Hier die Ausgaben des Programmes zu den Beispielen auf der BwInf-Webseite. Die gesamte Ausgabe ist hier \textbf{AusgabenBStrategie3.txt} \\ \newline \textbf{EDIT} \\ \newline zufinden. Zusätzlich ist hier die Ausführungszeit angegeben (intel i3-12100f, mit 32GB Ram).
\begin{enumerate}
  \item \textbf{schmuck1.txt}
  \begin{verbatim}
1. Preprocessing
   k = 25, C = 2, n=3, S0 = (0; 2, 1)
2a. Running Dynamic Programming (Dijkstra-like)
   Reached target finalState = (25; 0, 0) with cost 191
2b. DP Finished. Processed 319 states
   Total states in OPT table: 262
3. Minimum Total Cost (Sum Freq*Depth): 191
4. Reconstructing Code
   Reconstructing path
------------------------------
Execution time: 55 ms
------------------------------
{
  'e': 2 (Freq: 9, Länge: 2)
  't': 02 (Freq: 6, Länge: 3)
  '␣': 12 (Freq: 5, Länge: 3)
  'i': 000 (Freq: 3, Länge: 3)
  'n': 001 (Freq: 3, Länge: 3)
  'r': 010 (Freq: 3, Länge: 3)
  'w': 011 (Freq: 3, Länge: 3)
  'B': 012 (Freq: 2, Länge: 4)
  'I': 102 (Freq: 2, Länge: 4)
  '"': 002 (Freq: 2, Länge: 4)
  'b': 112 (Freq: 2, Länge: 4)
  'f': 1000 (Freq: 2, Länge: 4)
  's': 1001 (Freq: 2, Länge: 4)
  'D': 1010 (Freq: 1, Länge: 4)
  'F': 1011 (Freq: 1, Länge: 4)
  'N': 1100 (Freq: 1, Länge: 4)
  'W': 1101 (Freq: 1, Länge: 4)
  'a': 1002 (Freq: 1, Länge: 5)
  'd': 1012 (Freq: 1, Länge: 5)
  'h': 1102 (Freq: 1, Länge: 5)
  'k': 1112 (Freq: 1, Länge: 5)
  'm': 11100 (Freq: 1, Länge: 5)
  'o': 11101 (Freq: 1, Länge: 5)
  'u': 11110 (Freq: 1, Länge: 5)
  'ü': 11111 (Freq: 1, Länge: 5)
}

Gesamtlänge der Botschaft 191 (Anzahl in Perlen) bzw. 19.1cm

Kodierte Nachricht: 0121101102110010111210010221102021210001111101012002101000021
201211110001101221001011200002200112102001100011101010111001002020001112011202021
12201120101122002
  \end{verbatim}
  \item \textbf{schmuck2.txt}
  \begin{verbatim}
1. Preprocessing
   k = 9, C = 5, n=2, S0 = (0; 1, 0, 0, 0, 1)
2a. Running Dynamic Programming (Dijkstra-like)
   Reached target finalState = (9; 0, 0, 0, 0, 0) with cost 135
2b. DP Finished. Processed 246 states
   Total states in OPT table: 268
3. Minimum Total Cost (Sum Freq*Depth): 135
4. Reconstructing Code
   Reconstructing path
------------------------------
Execution time: 59 ms
------------------------------
{
  'a': 0 (Freq: 33, Länge: 1)
  '␣': 101 (Freq: 1, Länge: 11)
  'b': 110 (Freq: 1, Länge: 11)
  'g': 111 (Freq: 1, Länge: 15)
  'd': 1001 (Freq: 1, Länge: 12)
  'e': 10001 (Freq: 1, Länge: 13)
  'f': 100001 (Freq: 1, Länge: 14)
  'c': 1000000 (Freq: 1, Länge: 11)
  'h': 1000001 (Freq: 1, Länge: 15)
}

Gesamtlänge der Botschaft 135 (Anzahl in Perlen) bzw. 13.5cm

Kodierte Nachricht: 00000000000000000000000000000000010111010000001001100011000011
111000001
  \end{verbatim}
  \item \textbf{schmuck3.txt}
  \begin{verbatim}
1. Preprocessing
   k = 9, C = 3, n=3, S0 = (0; 1, 1, 1)
2a. Running Dynamic Programming (Dijkstra-like)
   Reached target finalState = (9; 0, 0, 0) with cost 279
2b. DP Finished. Processed 36 states
   Total states in OPT table: 46
3. Minimum Total Cost (Sum Freq*Depth): 279
4. Reconstructing Code
   Reconstructing path
------------------------------
Execution time: 43 ms
------------------------------
{
  'a': 1 (Freq: 34, Länge: 2)
  'c': 2 (Freq: 34, Länge: 3)
  'b': 00 (Freq: 34, Länge: 2)
  '␣': 02 (Freq: 3, Länge: 4)
  'd': 011 (Freq: 1, Länge: 5)
  'f': 012 (Freq: 1, Länge: 6)
  'e': 0100 (Freq: 1, Länge: 5)
  'g': 0101 (Freq: 1, Länge: 6)
  'h': 0102 (Freq: 1, Länge: 7)
}

Gesamtlänge der Botschaft 279 (Anzahl in Perlen) bzw. 27.9cm

Kodierte Nachricht: 11111111111111111111111111111111110200000000000000000000000000
0000000000000000000000000000000000000000000222222222222222222222222222222222220201
1010001201010102
  \end{verbatim}
  \item \textbf{schmuck4.txt}
  \begin{verbatim}
1. Preprocessing
   k = 14, C = 5, n=2, S0 = (0; 1, 0, 0, 0, 1)
2a. Running Dynamic Programming (Dijkstra-like)
   Reached target finalState = (14; 0, 0, 0, 0, 0) with cost 137
2b. DP Finished. Processed 1555 states
   Total states in OPT table: 1596
3. Minimum Total Cost (Sum Freq*Depth): 137
4. Reconstructing Code
   Reconstructing path
------------------------------
Execution time: 68 ms
------------------------------
{
  'g': 11 (Freq: 1, Länge: 10)
  'a': 001 (Freq: 1, Länge: 7)
  'i': 011 (Freq: 1, Länge: 11)
  'j': 101 (Freq: 1, Länge: 11)
  'b': 0001 (Freq: 1, Länge: 8)
  'c': 0100 (Freq: 1, Länge: 8)
  'd': 1000 (Freq: 1, Länge: 8)
  'l': 0101 (Freq: 1, Länge: 12)
  'm': 1001 (Freq: 1, Länge: 12)
  'f': 00001 (Freq: 1, Länge: 9)
  'h': 000001 (Freq: 1, Länge: 10)
  'k': 0000001 (Freq: 1, Länge: 11)
  'e': 00000000 (Freq: 1, Länge: 8)
  'n': 00000001 (Freq: 1, Länge: 12)
}

Gesamtlänge der Botschaft 137 (Anzahl in Perlen) bzw. 13.7cm

Kodierte Nachricht: 0010001010010000000000000001110000010111010000001010110010000
0001
  \end{verbatim}
  \item \textbf{schmuck5.txt}
  \begin{verbatim}
1. Preprocessing
   k = 41, C = 6, n=7, S0 = (0; 2, 1, 1, 1, 1, 1)
2a. Running Dynamic Programming (Dijkstra-like)
   Reached target finalState = (41; 0, 0, 0, 0, 0, 0) with cost 3162
2b. DP Finished. Processed 485 states
   Total states in OPT table: 1068
3. Minimum Total Cost (Sum Freq*Depth): 3162
4. Reconstructing Code
   Reconstructing path
------------------------------
Execution time: 105 ms
------------------------------
{
  '␣': 2 (Freq: 151, Länge: 2)
  'e': 00 (Freq: 110, Länge: 2)
  't': 3 (Freq: 71, Länge: 3)
  'i': 02 (Freq: 67, Länge: 3)
  'o': 12 (Freq: 64, Länge: 3)
  's': 010 (Freq: 61, Länge: 3)
  'a': 011 (Freq: 58, Länge: 3)
  'n': 100 (Freq: 56, Länge: 3)
  'r': 101 (Freq: 51, Länge: 3)
  'c': 110 (Freq: 44, Länge: 3)
  'd': 111 (Freq: 37, Länge: 3)
  'l': 4 (Freq: 33, Länge: 4)
  'm': 03 (Freq: 27, Länge: 4)
  'h': 13 (Freq: 26, Länge: 4)
  'u': 012 (Freq: 25, Länge: 4)
  'f': 102 (Freq: 20, Länge: 4)
  'p': 112 (Freq: 20, Länge: 4)
  'b': 5 (Freq: 16, Länge: 5)
  'y': 04 (Freq: 13, Länge: 5)
  'g': 14 (Freq: 11, Länge: 5)
  '.': 013 (Freq: 8, Länge: 5)
  'v': 6 (Freq: 5, Länge: 6)
  'x': 05 (Freq: 5, Länge: 6)
  ',': 103 (Freq: 5, Länge: 5)
  'q': 113 (Freq: 5, Länge: 5)
  'w': 15 (Freq: 4, Länge: 6)
  'F': 014 (Freq: 3, Länge: 6)
  'T': 104 (Freq: 3, Länge: 6)
  '-': 06 (Freq: 1, Länge: 7)
  '1': 16 (Freq: 1, Länge: 7)
  'A': 106 (Freq: 1, Länge: 8)
  'G': 116 (Freq: 1, Länge: 8)
  '2': 015 (Freq: 1, Länge: 7)
  '5': 105 (Freq: 1, Länge: 7)
  '9': 115 (Freq: 1, Länge: 7)
  ';': 016 (Freq: 1, Länge: 8)
  'H': 1142 (Freq: 1, Länge: 8)
  'S': 11400 (Freq: 1, Länge: 8)
   '’': 11411 (Freq: 1, Länge: 8)
  'j': 11401 (Freq: 1, Länge: 8)
  'z': 11410 (Freq: 1, Länge: 8)
}

Gesamtlänge der Botschaft 3162 (Anzahl in Perlen) bzw. 316.2cm

Kodierte Nachricht: 114000120303011101042104130201021120111120010121402600010201120
30031312111210212101211012100010310101211030210014203021000203012030610100111012100
11101110011004211210100102020521101211100010210212101231300214001000010101142111020
10110101003002100120201000400010010211013011100100004215023131201232110121000103101
01102100301001321041300211012010301021210221101211100240033001010102100000011121001
23250020011301201141032011100111231300201004035124010200100110121110011120111010021
00123201101001001203001112312250020011301201144042112101125011540001321062010124012
30212100213011111211210100602120120104042500001002140260010025042114201210210203011
10020210021611510501521021210123130020101120011002011421100110100020210021513021101
32011442110121110024003300101010201110100212102200113012011421101201030132104130021
12101000100010032111006004121120300100320201020114140051010110211001320140210101031
03201031010121103012101002102012100110302121000102011101002111001020210000111103202
10023001010301021210221513021101321000011000010010011101042011100111201001210210202
11002001003211012100111023021210001021021210123130020005020103001001100021210221121
01001020205211012111000102030110425002010301130011101320141011203231300010002110121
00111023021210001010324021000001110120210000113012011402302000102011101002111001010
26001112151302110132030110425002012010001112312211013011101011110300101021141000211
21010010202052110121110001001321161203121010411411010202100300140010121121011214101
01103030210014201141412101023130320201023130010020120100011123122110121000103101012
11032121123020301203211012111000102010012511401001103231223130001000202100001130120
11402302000100162011100200400140111003211012035021000113121010201142011112112101120
11110132312212530110210020112010310102110340421101203112012301130212100011420005112
00101020010011000202010211210100010001003001112312211100031210001031010113002313002
11210101111030211001150240230421210223130020300313121110132014021000114404103201012
03002011111111023021210001142110121110210014211210112540003010201110100211102010110
01201001000111201110011120112112101125400032121022110401101001002102021100113021210
02020102310100011300111013
  \end{verbatim}
  Dieses Ergebnis (3162) ist der Vergleichswert (3162) von der BwInf Webseite, dies bestätigt die Vermutung, dass der angegebene Wert die optimale Kodierungslänge ist :)

  \item \textbf{schmuck6.txt}
  \\
  Ich habe es nicht geschafft, dass Unicodezeichen hier in einem verbatim in latex angezeigt werden, daher sind die Zeichen leer. In der Textdatei mit den gesamten Ausgaben, sind diese Zeichen sichtbar.
  \begin{verbatim}
1. Preprocessing
   k = 34, C = 3, n=3, S0 = (0; 1, 1, 1)
2a. Running Dynamic Programming (Dijkstra-like)
   Reached target finalState = (34; 0, 0, 0) with cost 234
2b. DP Finished. Processed 2496 states
   Total states in OPT table: 1892
3. Minimum Total Cost (Sum Freq*Depth): 234
4. Reconstructing Code
   Reconstructing path
------------------------------
Execution time: 99 ms
------------------------------
{
  '': 02 (Freq: 3, Länge: 4)
  '': 12 (Freq: 2, Länge: 5)
  '': 21 (Freq: 2, Länge: 5)
  '': 002 (Freq: 2, Länge: 5)
  '': 011 (Freq: 2, Länge: 5)
  '': 22 (Freq: 1, Länge: 6)
  '': 101 (Freq: 1, Länge: 5)
  '': 110 (Freq: 1, Länge: 5)
  '': 200 (Freq: 1, Länge: 5)
  '': 112 (Freq: 1, Länge: 7)
  '': 102 (Freq: 1, Länge: 6)
  '': 111 (Freq: 1, Länge: 6)
  '': 202 (Freq: 1, Länge: 7)
  '': 012 (Freq: 1, Länge: 6)
  '': 201 (Freq: 1, Länge: 6)
  '': 1001 (Freq: 1, Länge: 6)
  '': 1002 (Freq: 1, Länge: 7)
  '': 0011 (Freq: 1, Länge: 6)
  '': 0101 (Freq: 1, Länge: 6)
  '': 0012 (Freq: 1, Länge: 7)
  '': 0102 (Freq: 1, Länge: 7)
  '': 0002 (Freq: 1, Länge: 6)
  '': 01001 (Freq: 1, Länge: 7)
  '': 00001 (Freq: 1, Länge: 6)
  '': 00010 (Freq: 1, Länge: 6)
  '': 10001 (Freq: 1, Länge: 7)
  '': 01000 (Freq: 1, Länge: 6)
  '': 10000 (Freq: 1, Länge: 6)
  '': 00101 (Freq: 1, Länge: 7)
  '': 00100 (Freq: 1, Länge: 6)
  '': 00002 (Freq: 1, Länge: 7)
  '': 00011 (Freq: 1, Länge: 7)
  '': 000001 (Freq: 1, Länge: 7)
  '': 000000 (Freq: 1, Länge: 6)
}

Gesamtlänge der Botschaft 234 (Anzahl in Perlen) bzw. 23.4cm

Kodierte Nachricht: 10201020100100100100210010200011000000111010102100010100000101202
00112100021010111000000010112000022001200001212200220100201102110001201200000112
  \end{verbatim}
  \item \textbf{schmuck7.txt}
\begin{verbatim}
1. Preprocessing
   k = 82, C = 4, n=10, S0 = (0; 7, 1, 1, 1)
2a. Running Dynamic Programming (Dijkstra-like)
   Reached target finalState = (82; 0, 0, 0, 0) with cost 134559
2b. DP Finished. Processed 205 states
   Total states in OPT table: 476
3. Minimum Total Cost (Sum Freq*Depth): 134559
4. Reconstructing Code
   Reconstructing path
------------------------------
Execution time: 270 ms
------------------------------
{
  '␣': 0 (Freq: 13488, Länge: 1)
  'e': 1 (Freq: 11480, Länge: 1)
  'n': 2 (Freq: 7761, Länge: 1)
  'i': 3 (Freq: 5018, Länge: 1)
  'r': 7 (Freq: 4011, Länge: 2)
  's': 40 (Freq: 3507, Länge: 2)
  'a': 41 (Freq: 3307, Länge: 2)
  'd': 42 (Freq: 3272, Länge: 2)
  'h': 43 (Freq: 3213, Länge: 2)
  't': 44 (Freq: 3174, Länge: 2)
  'u': 45 (Freq: 2609, Länge: 2)
  'l': 46 (Freq: 2448, Länge: 2)
  'c': 50 (Freq: 1991, Länge: 2)
  'g': 51 (Freq: 1665, Länge: 2)
  'm': 52 (Freq: 1521, Länge: 2)
  'o': 53 (Freq: 1304, Länge: 2)
  'b': 54 (Freq: 1141, Länge: 2)
  ',': 55 (Freq: 1077, Länge: 2)
  'f': 56 (Freq: 1000, Länge: 2)
  'w': 60 (Freq: 941, Länge: 2)
  '.': 61 (Freq: 831, Länge: 2)
  'k': 62 (Freq: 798, Länge: 2)
  'z': 63 (Freq: 631, Länge: 2)
  'S': 8 (Freq: 498, Länge: 3)
  'ä': 47 (Freq: 455, Länge: 3)
  'ü': 57 (Freq: 434, Länge: 3)
  'ö': 67 (Freq: 399, Länge: 3)
  'v': 640 (Freq: 361, Länge: 3)
  'F': 641 (Freq: 300, Länge: 3)
  'p': 642 (Freq: 300, Länge: 3)
  'ß': 643 (Freq: 278, Länge: 3)
  'D': 644 (Freq: 277, Länge: 3)
  'E': 645 (Freq: 243, Länge: 3)
  'V': 646 (Freq: 242, Länge: 3)
  'W': 650 (Freq: 231, Länge: 3)
  'N': 651 (Freq: 218, Länge: 3)
  'K': 652 (Freq: 206, Länge: 3)
  'B': 653 (Freq: 193, Länge: 3)
  'A': 654 (Freq: 159, Länge: 3)
  'M': 655 (Freq: 159, Länge: 3)
  'H': 656 (Freq: 152, Länge: 3)
  'I': 660 (Freq: 134, Länge: 3)
  'L': 661 (Freq: 126, Länge: 3)
  'R': 662 (Freq: 115, Länge: 3)
  'G': 663 (Freq: 106, Länge: 3)
  'T': 664 (Freq: 102, Länge: 3)
  'Z': 9 (Freq: 79, Länge: 4)
  'j': 48 (Freq: 72, Länge: 4)
  'J': 58 (Freq: 68, Länge: 4)
  '-': 68 (Freq: 66, Länge: 4)
  ';': 647 (Freq: 37, Länge: 4)
  '(': 657 (Freq: 32, Länge: 4)
  ')': 667 (Freq: 32, Länge: 4)
  ':': 6650 (Freq: 28, Länge: 4)
  '_': 6651 (Freq: 28, Länge: 4)
  'O': 6652 (Freq: 27, Länge: 4)
  'U': 6653 (Freq: 25, Länge: 4)
  '[': 6654 (Freq: 24, Länge: 4)
  ']': 6655 (Freq: 24, Länge: 4)
  '“': 6656 (Freq: 21, Länge: 4)
  '„': 6660 (Freq: 21, Länge: 4)
  'P': 6661 (Freq: 20, Länge: 4)
  '1': 6662 (Freq: 16, Länge: 4)
  '4': 6663 (Freq: 13, Länge: 4)
  '3': 6664 (Freq: 9, Länge: 4)
  '?': 6665 (Freq: 8, Länge: 4)
  '!': 49 (Freq: 7, Länge: 5)
  '2': 59 (Freq: 7, Länge: 5)
  '5': 69 (Freq: 7, Länge: 5)
  '6': 648 (Freq: 6, Länge: 5)
  '0': 658 (Freq: 5, Länge: 5)
  '8': 668 (Freq: 3, Länge: 5)
  'Ä': 6667 (Freq: 3, Länge: 5)
  'q': 6657 (Freq: 3, Länge: 5)
   '’': 66663 (Freq: 2, Länge: 5)
  '7': 66660 (Freq: 2, Länge: 5)
  '9': 66661 (Freq: 2, Länge: 5)
  'Q': 66662 (Freq: 2, Länge: 5)
  'Ö': 649 (Freq: 1, Länge: 6)
  'C': 66664 (Freq: 1, Länge: 5)
  'x': 66665 (Freq: 1, Länge: 5)
  'y': 66666 (Freq: 1, Länge: 5)
}

Gesamtlänge der Botschaft 134559 (Anzahl in Perlen) bzw. 13455.9cm

Kodierte Nachricht: 654454004215206611541204524017170646675114661066116244353206... (die kodierte Nachricht ist mehr als 110KB)
\end{verbatim}

\item \textbf{schmuck8.txt}
\begin{verbatim}
1. Preprocessing
   k = 321, C = 3, n=5, S0 = (0; 2, 2, 1)
2a. Running Dynamic Programming (Dijkstra-like)
   Processed 30000 states (pop ops), PQ size 372225
   Processed 60000 states (pop ops), PQ size 664988
   Processed 90000 states (pop ops), PQ size 803740
    ... 
   Processed 900000 states (pop ops), PQ size 674085
   Processed 930000 states (pop ops), PQ size 644298
   Processed 990000 states (pop ops), PQ size 586335
   Processed 1110000 states (pop ops), PQ size 468469
   Processed 1170000 states (pop ops), PQ size 408474
   Reached target finalState = (321; 0, 0, 0) with cost 3287
2b. DP Finished. Processed 1188509 states
   Total states in OPT table: 1059938
3. Minimum Total Cost (Sum Freq*Depth): 3287
4. Reconstructing Code
   Reconstructing path
------------------------------
Execution time: 38.298 s
------------------------------
{
  '': 4 (Freq: 33, Länge: 3)
  '': 02 (Freq: 27, Länge: 3)
  '': 04 (Freq: 13, Länge: 4)
  '': 14 (Freq: 13, Länge: 4)
  '': 22 (Freq: 12, Länge: 4)
  '': 23 (Freq: 11, Länge: 4)
  '': 32 (Freq: 11, Länge: 4)
  '': 33 (Freq: 10, Länge: 4)
  '': 002 (Freq: 10, Länge: 4)
  '': 003 (Freq: 9, Länge: 4)
  '': 012 (Freq: 7, Länge: 4)
  '': 030 (Freq: 7, Länge: 4)
  '': 013 (Freq: 7, Länge: 4)
  '': 34 (Freq: 6, Länge: 5)
  '': 24 (Freq: 6, Länge: 5)
  '': 031 (Freq: 6, Länge: 4)
  '': 033 (Freq: 5, Länge: 5)
  '': 104 (Freq: 5, Länge: 5)
  '': 004 (Freq: 5, Länge: 5)
  '': 032 (Freq: 5, Länge: 5)
  '': 014 (Freq: 5, Länge: 5)
  '': 132 (Freq: 4, Länge: 5)
  '': 114 (Freq: 4, Länge: 5)
  '': 202 (Freq: 4, Länge: 5)

  ...

 
  '': 000113 (Freq: 1, Länge: 7)
  '': 110001 (Freq: 1, Länge: 6)
  '': 101101 (Freq: 1, Länge: 6)
  '': 100100 (Freq: 1, Länge: 6)
  '': 010000 (Freq: 1, Länge: 6)
  '': 000000 (Freq: 1, Länge: 6)
  '': 110111 (Freq: 1, Länge: 6)
  '': 101003 (Freq: 1, Länge: 7)
  '': 101012 (Freq: 1, Länge: 7)
  '': 011111 (Freq: 1, Länge: 6)
  '': 011112 (Freq: 1, Länge: 7)
  '': 101013 (Freq: 1, Länge: 7)
  '': 101102 (Freq: 1, Länge: 7)
  '': 011113 (Freq: 1, Länge: 7)
  '': 000001 (Freq: 1, Länge: 6)
  '': 000002 (Freq: 1, Länge: 7)
  '': 000010 (Freq: 1, Länge: 6)
  '': 001002 (Freq: 1, Länge: 7)
  '': 001010 (Freq: 1, Länge: 6)
  '': 101110 (Freq: 1, Länge: 6)
  '': 000003 (Freq: 1, Länge: 7)
  '': 000011 (Freq: 1, Länge: 6)
  '': 010110 (Freq: 1, Länge: 6)
  '': 110112 (Freq: 1, Länge: 7)
}

Gesamtlänge der Botschaft 3287 (Anzahl in Perlen) bzw. 328.7cm

Kodierte Nachricht: 010020001031100030103100003344102131420103010411321101011001130...
\end{verbatim}
\item \textbf{schmuck9.txt}

\begin{verbatim}
1. Preprocessing
   k = 674, C = 4, n=4, S0 = (0; 1, 1, 1, 1)
2a. Running Dynamic Programming (Dijkstra-like)
   Processed 30000 states (pop ops), PQ size 257044
   Processed 60000 states (pop ops), PQ size 666081
   Processed 90000 states (pop ops), PQ size 1315271
   Processed 120000 states (pop ops), PQ size 1524169
   Processed 150000 states (pop ops), PQ size 1631613

    ...

   Processed 7230000 states (pop ops), PQ size 56443913
   Processed 7260000 states (pop ops), PQ size 56748051
   Processed 7290000 states (pop ops), PQ size 57033159
   Processed 7320000 states (pop ops), PQ size 57361366
   Processed 7350000 states (pop ops), PQ size 57750525
   Processed 7380000 states (pop ops), PQ size 58057696
   Processed 7410000 states (pop ops), PQ size 58362853
   Processed 7440000 states (pop ops), PQ size 58707514

   Ab hier habe ich abgebrochen. 
\end{verbatim}
Diese Lösungsansatz ist mit Werten von $k = 674, C = 4, n=4, S_0 = (0; 1, 1, 1, 1)$ und einer Laufzeit von $\mathcal{O}(C\cdot k^{C+2})=\mathcal{O}(4\cdot674^{6})$ und mit einem Speicherbedarf von $\mathcal{O}(4\cdot 674^{5})$ nicht mehr realistisch. Der benötigte Speicher wäre im Größenbereich von Petabytes an RAM, (aber hängt davon ab wie viele Zustände tatsächlich erreicht werden: Landau-Notation ist nur eine obere Schranke)
\end{enumerate}

\subsubsection{Quellcode}
\textbf{Wichtige Teile des Quellcodes:}
\\\newline
\textbf{Berechnung des characteristischen Vektors d}
\begin{lstlisting}
// Calculate characteristic vector d and max cost C
private static CharacteristicVectorResult calculateCharacteristicVector(List<Integer> costs) {
    if (costs == null || costs.isEmpty()) {
        return new CharacteristicVectorResult(new int[0], 0);
    }
    // Find max C
    int maxC = 0;
    for (int cost : costs) {
        if (cost > 0) {
            maxC = Math.max(maxC, cost);
        }
    }
    if (maxC == 0) {
        return new CharacteristicVectorResult(new int[0], 0);
    }

    // Count occurrences of every positive cost up to maxC
    int[] dCounts = new int[maxC + 1]; // dCounts[i] store count for cost i
    for (int cost : costs) {
        if (cost > 0) {
            dCounts[cost]++;
        }
    }
    // Create final 'd' array which starts from cost 1
    int[] dResult = Arrays.copyOfRange(dCounts, 1, maxC + 1);
    return new CharacteristicVectorResult(dResult, maxC);
}
\end{lstlisting}

\textbf{Hauptmethode mit DP}
\begin{lstlisting}
// Main solver method
private static OptimalCodingResult solveOptimalCode(String text, List<Integer> alphabetCosts) {
    // 1. Preprocessing step
    System.out.println("1. Preprocessing");
    // 1a. Calculate symbol frequencies
    FrequencyResult freqResult = calculateFrequencies(text);
    if (freqResult.sortedSymbols.isEmpty()) {
        return new OptimalCodingResult(Collections.emptyMap(), 0, null, null);
    }

    // Num of unique symbols k
    int k = freqResult.sortedSymbols.size();
    // Frequency counts
    long[] freqCounts = freqResult.sortedSymbols.stream().mapToLong(Map.Entry::getValue).toArray();

    // 1b. Precompute cumulative frequency counts (suffix sums)
    // Needed for edge cost calculation in O(1)
    long[] cumFreq = new long[k + 1];
    // cumFreq[i] = sum of frequencies from index i to n-1 (i.e., P_{i+1} + ... +
    // P_n)
    for (int i = k - 1; i >= 0; i--) {
        cumFreq[i] = cumFreq[i + 1] + freqCounts[i];
    }

    // 1c. Calculate characteristic vector 'd' and maximum cost 'C'
    CharacteristicVectorResult charVecResult = calculateCharacteristicVector(alphabetCosts);
    int[] d = charVecResult.d;
    int C = charVecResult.C;
    if (C == 0) {
        System.err.println("Error: No valid alphabet costs provided (C=0).");
        return new OptimalCodingResult(null, Long.MAX_VALUE, "Invalid alphabet costs", null);
    }
    int r = alphabetCosts.size();

    // 2. Build Zustandsgraph
    // Explore signatures to find min cost path
    // Simulate building optimal code tree top-down

    // 2a. Init DP table (OPT) and Predecessor table (Pred)
    // Saves minimal cost to get to a signature from S0
    Map<Signature, Long> opt = new HashMap<>();
    // Saves the current best way to get to a signature
    Map<Signature, PredecessorInfo> pred = new HashMap<>();

    // Calculate initial signature S0
    // List full of 0
    List<Integer> initialLsRaw = new ArrayList<>(Collections.nCopies(C, 0));
    for (int i = 0; i < d.length; ++i) {
        // Set initial to values of characteristic vector
        initialLsRaw.set(i, d[i]); // d has length C
    }
    // Signature of "root"
    Signature initialSigRaw = new Signature(0, initialLsRaw);
    Signature S0 = reduceSignature(initialSigRaw, k);
    opt.put(S0, 0L);

    // Target sig: all Symbols in the tree have a codeword
    Signature finalState = new Signature(k, Collections.nCopies(C, 0));

    // Priority Queue for Dijkstra like (process states with lowest cost first)
    PriorityQueue<StateCost> pq = new PriorityQueue<>();
    pq.add(new StateCost(0L, S0));

    System.out.println("   n = " + k + ", C = " + C + ", r=" + r + ", S0 = " + S0);
    System.out.println("2a. Running Dynamic Programming (Dijkstra-like)");
    long processedCount = 0;
    // Store the actual final signature, to begin null, check after dp, if still
    // null
    Signature actualFinalSig = null;
    // Store the final cost to get from S0 to final State
    long finalCostFound = Long.MAX_VALUE;

    // 2b. Main DP loop
    while (!pq.isEmpty()) {
        // Get state with smallest cost from queue
        StateCost current = pq.poll();
        long currentCost = current.cost;
        Signature currentSig = current.signature;
        processedCount++;

        // If found a shorter path to a signature already, skip
        if (currentCost > opt.getOrDefault(currentSig, Long.MAX_VALUE)) {
            continue;
        }

        // Check if reached target state
        if (currentSig.equals(finalState)) {
            if (currentCost < finalCostFound) {
                finalCostFound = currentCost;
                actualFinalSig = currentSig;
                System.out.println("   Reached target finalState = " + currentSig + " with cost " + currentCost);
                break;
            }
        }

        // Small optimization
        if (actualFinalSig != null && currentCost >= finalCostFound)
            continue;

        // Logging
        if (processedCount % 30000 == 0) {
            System.out.println("   Processed " + processedCount + " states (pop ops), PQ size " + pq.size());
        }

        // --- Generate next states
        // Get data from current signature
        int m = currentSig.m;
        int l1 = currentSig.getL(0);

        // Calculate cost added for expansion step (edge weight in state graph)
        long edgeCost = (m < k) ? cumFreq[m] : 0;
        long newBaseCost = currentCost + edgeCost;

        // Iterate through possible expansions q
        for (int q = 0; q <= l1; q++) {
            // Get signature for an expansion
            Signature nextSigRaw = getNextSigRaw(currentSig, q, d, C);
            Signature nextSignature = reduceSignature(nextSigRaw, k);

            // Update if a better path is found
            if (newBaseCost < opt.getOrDefault(nextSignature, Long.MAX_VALUE)) {
                // Better cost was found to get to a signature
                opt.put(nextSignature, newBaseCost);
                // Store how to get to that signature
                pred.put(nextSignature, new PredecessorInfo(currentSig, q));
                // Add a new state to explore to pq
                pq.add(new StateCost(newBaseCost, nextSignature));
            }
        }
    }

    System.out.println("2b. DP Finished. Processed " + processedCount + " states");
    System.out.println("   Total states in OPT table: " + opt.size());

    // 3. Get result and check if finalState was reached
    if (actualFinalSig == null) {
        // Check if anystate (n, 0, ..., 0) was reached if finalState was not hit
        // directly
        boolean foundEquivalent = false;
        // Iterate over all signatures in opt and check if state was reached that is
        // equivalent
        for (Map.Entry<Signature, Long> entry : opt.entrySet()) {
            Signature sig = entry.getKey();
            // Check if m == n and all l_i == 0
            if (sig.m == k && sig.list.stream().allMatch(l -> l == 0)) {
                if (entry.getValue() < finalCostFound) {
                    finalCostFound = entry.getValue();
                    actualFinalSig = sig;
                    foundEquivalent = true;
                }
            }
        }
        if (foundEquivalent) {
            System.out.println(
                    "   Reached equivalent final state " + actualFinalSig + " with cost " + finalCostFound);
        } else {
            System.out.println("Error: Target signature finalState not reached");
            return new OptimalCodingResult(null, Long.MAX_VALUE, "Target state not reachable", null);
        }
    }

    // Optimal cost found by DP
    long minTotalCost = finalCostFound;
    System.out.println("3. Minimum Total Cost (Sum Freq*Depth): " + minTotalCost);

    // 4. Reconstruct code
    System.out.println("4. Reconstructing Code");
    // Reconstruct path and determine leaf depths
    // Simulates the optimal tree construction process based on pred map
    Map<Integer, Long> leafDepthsCounts = getLeafDepthsFromPath(actualFinalSig, pred, S0, k, d, alphabetCosts);

    // Error
    if (leafDepthsCounts == null) {
        System.out.println("  Error:( Failed to reconstruct leaf depths");
        return new OptimalCodingResult(null, minTotalCost, "Leaf depth reconstruction failed", null);
    }

    // 5b. Create actual code table using determined dpeths
    Map<Character, String> codeTable = buildCodeFromDepths(freqResult.sortedSymbols, leafDepthsCounts,
            alphabetCosts);

    if (codeTable == null) {
        System.err.println("   Failed to assign codewords from depths.");
        return new OptimalCodingResult(null, minTotalCost, "Codeword assignment failed", null);
    }

    return new OptimalCodingResult(codeTable, minTotalCost, null, freqResult.counts);
}
\end{lstlisting}
\textbf{Rekonstruktion der Blatttiefen}
\begin{lstlisting}
// ------------- Reconstruction of tree and Code Assignment

// Step 4
// Trace back path from finalSig to S0 and simulate tree construction to find
// leaf depths
// Then simulates tree construction process (forward) along this optimal path to
// find exact depth at wich each of the n leaves is finalized
private static Map<Integer, Long> getLeafDepthsFromPath(
        Signature finalSig, Map<Signature, PredecessorInfo> pred, Signature S0,
        int n, int[] characVector, List<Integer> alphabetCosts) {
    System.out.println("   Reconstructing path");
    List<PredecessorInfo> path = new ArrayList<>();
    Signature currentSignature = finalSig;

    // 4a. Backtrack from final state to S0 with pred map
    while (!currentSignature.equals(S0)) {
        PredecessorInfo info = pred.get(currentSignature);
        if (info == null) {
            System.out.println(
                    "   Error: Cannot backtrack from signature " + currentSignature + "; failed");
            return null;
        }
        // Add info about step to current Sign
        path.add(info);
        currentSignature = info.previousSignature;
    }
    // Path currently in reverse order (final -> S0), reverse here for forward
    // simulation
    Collections.reverse(path);

    if (path.isEmpty() && S0.equals(finalSig)) {
        // Handle n=0 or n=1 case where path might be empty
        return (S0.m == n) ? new HashMap<>() : null;
    }

    // 4b. --- Simulate forward construction
    // Store final result
    Map<Integer, Long> leavesFinishedAtDepth = new HashMap<>();
    // Track currently open end (potentiall leaves) and their depths during
    // simulation
    Map<Integer, Long> activeLeaves = new HashMap<>();

    // Init active leaves based on initial state S0 (after reduction)
    // Need to know wich children root has according to S0
    Signature unreducedS0 = new Signature(0, Arrays.stream(characVector).boxed().collect(Collectors.toList()));
    if (S0.equals(unreducedS0)) {
        for (int cost : alphabetCosts) {
            if (cost > 0)
                activeLeaves.put(cost, activeLeaves.getOrDefault(cost, 0L) + 1);
        }
    } else {
        Map<Integer, Long> tempActive = new HashMap<>();
        for (int cost : alphabetCosts) {
            if (cost > 0)
                tempActive.put(cost, tempActive.getOrDefault(cost, 0L) + 1);
        }
        long currentTotal = 0;
        List<Integer> sortedDepths = new ArrayList<>(tempActive.keySet());
        Collections.sort(sortedDepths);
        for (int depth : sortedDepths) {
            long countAtDepth = tempActive.get(depth);
            long canKeep = n - currentTotal;
            long keep = Math.min(countAtDepth, canKeep);
            if (keep > 0) {
                activeLeaves.put(depth, keep);
                currentTotal += keep;
            }
            if (currentTotal >= n)
                break;
        }
    }

    // Simulate forward along path
    int currentLevel = 0; // Track implicit level of signatures
    for (PredecessorInfo info : path) {
        // Optimal expansion for this step
        int q = info.q;
        int simDepth = currentLevel + 1;

        long numActiveAtSimDepth = activeLeaves.getOrDefault(simDepth, 0L);

        // Determine how many nodes at simDepth expand or finalize
        long nodesToExpand = Math.min(q, numActiveAtSimDepth);
        long nodesToFinalize = numActiveAtSimDepth - nodesToExpand;

        // Update active leaves
        activeLeaves.remove(simDepth);

        // Add children of expanded nodes to active leaves
        for (int i = 0; i < nodesToExpand; i++) {
            for (int cost : alphabetCosts) {
                if (cost > 0)
                    activeLeaves.put(simDepth + cost, activeLeaves.getOrDefault(simDepth + cost, 0L) + 1);
            }
        }

        // Simulate Reduction
        long currentFinishedCount = leavesFinishedAtDepth.values().stream().mapToLong(Long::longValue).sum();
        long currentActiveCount = activeLeaves.values().stream().mapToLong(Long::longValue).sum();
        long potentialTotal = currentFinishedCount + currentActiveCount + nodesToFinalize;
        // How man leaves to remove to stay in n
        long leavesToRemove = Math.max(0, potentialTotal - n);

        // Remove deepest leaves first
        long removedCount = 0;
        if (leavesToRemove > 0) {
            List<Integer> sortedActiveDepths = new ArrayList<>(activeLeaves.keySet());
            sortedActiveDepths.sort(Collections.reverseOrder()); // Deepest first, -> takes most of runtime when
                                                                  // timecomplexity is viewed
            for (int depth : sortedActiveDepths) {
                // Removed enough
                if (removedCount >= leavesToRemove)
                    break; //
                long canRemoveAtDepth = activeLeaves.get(depth);
                long removeNow = Math.min(leavesToRemove - removedCount, canRemoveAtDepth);
                activeLeaves.put(depth, activeLeaves.get(depth) - removeNow);
                if (activeLeaves.get(depth) == 0) {
                    activeLeaves.remove(depth);
                }
                removedCount += removeNow;
            }
        }

        long nodesFinalizedThisStep = Math.max(0, nodesToFinalize - (leavesToRemove - removedCount));

        if (nodesFinalizedThisStep > 0) {
            leavesFinishedAtDepth.put(simDepth,
                    leavesFinishedAtDepth.getOrDefault(simDepth, 0L) + nodesFinalizedThisStep);
        }
        // Move to next level
        currentLevel++;
    }

    // After loop any remaining active leaves are finalized leaves
    for (Map.Entry<Integer, Long> entry : activeLeaves.entrySet()) {
        leavesFinishedAtDepth.put(entry.getKey(),
                leavesFinishedAtDepth.getOrDefault(entry.getKey(), 0L) + entry.getValue());
    }

    // Verify
    long totalLeavesReconstructed = leavesFinishedAtDepth.values().stream().mapToLong(Long::longValue).sum();
    if (totalLeavesReconstructed != n) {
        System.err.println(
                "   Warning: Reconstructed " + totalLeavesReconstructed + " leaves, but expected " + n);
    }

    return leavesFinishedAtDepth;
}
\end{lstlisting}
\textbf{Generierung der Codetabelle}
\begin{lstlisting}
// 5. Construct prefix free code
public static Map<Character, String> buildCodeFromDepths(
        List<Map.Entry<Character, Long>> symbolFreqs,
        Map<Integer, Long> depthsCounts,
        List<Integer> costs) {
    if (symbolFreqs == null || symbolFreqs.isEmpty() || depthsCounts == null || depthsCounts.isEmpty()) {
        return Collections.emptyMap();
    }

    int n = symbolFreqs.size();
    int r = costs.size();
    List<Character> symbols = symbolFreqs.stream().map(Map.Entry::getKey).collect(Collectors.toList());

    // Create and sort targets
    // Every entry represents one required leaf: tragetDepth, symbolIndex
    List<TargetInfo> targets = new ArrayList<>();
    int symbolIdx = 0;
    int maxTargetDepth = 0;
    List<Integer> sortedDepths = new ArrayList<>(depthsCounts.keySet());
    Collections.sort(sortedDepths);

    for (int depth : sortedDepths) {
        // How many leaves are needed at this depth
        long count = depthsCounts.get(depth);
        maxTargetDepth = Math.max(maxTargetDepth, depth);
        for (int i = 0; i < count; i++) {
            if (symbolIdx < n) {
                targets.add(new TargetInfo(depth, symbolIdx));
                symbolIdx++;
            } else {
                System.out.println("Error: More depths specified than symbols");
                break;
            }
        }
        if (symbolIdx >= n)
            break;
    }

    // Sort by depth, then symbol index
    Collections.sort(targets);

    // Generate and assing codes greedily
    Map<Character, String> codeMap = new HashMap<>();
    List<String> assignedCodesList = new ArrayList<>(); // For prefix checks

    // Use queue for BFS like generation of candiate codewords
    Deque<String> generatorQueue = new ArrayDeque<>();
    // Start with single digit codes (children of root)
    for (int i = 0; i < r; i++) {
        generatorQueue.offerLast(String.valueOf(i));
    }
    // Pool to store codewords that have wrong cost for current target but match
    // maybe later
    Map<Integer, Deque<String>> candidatePool = new HashMap<>();

    // Maximum generation limit to prevent infinite loops
    int minCost = Integer.MAX_VALUE;
    for (int cost : costs) {
        if (cost > 0) {
            minCost = Math.min(minCost, cost);
        }
    }
    if (minCost == Integer.MAX_VALUE) {
        minCost = 1; // Avoid durch null teilen
    }

    // Estimate max length needed plus 2 buffer
    int worstCaseLen = (maxTargetDepth > 0) ? (maxTargetDepth / minCost + 2) : 1;
    // Estimate of max nodes in full tree of that length and set cap
    long MaxGeneratedNodes = (long) Math.min(10000000, Math.max(n * 10L, (long) Math.pow(r, worstCaseLen + 1)));
    long generatedCount = 0;

    // Iterate through targets and assing next available code
    for (TargetInfo target : targets) {
        int targetDepth = target.depth;
        int targetSymbolIndex = target.symbolIndex;
        char targetSymbol = symbols.get(targetSymbolIndex);
        boolean foundCodeForTarget = false;

        // 1. Check candidate pool
        Deque<String> poolDeque = candidatePool.get(targetDepth);
        if (poolDeque != null && !poolDeque.isEmpty()) {
            // Temp store codes that fail prefix check
            Deque<String> skipped = new ArrayDeque<>();
            while (!poolDeque.isEmpty()) {
                String candidate = poolDeque.pollFirst();
                boolean isPrefixFree = true;
                // Check if it acually is prefix free with already assigned codes
                for (String assigned : assignedCodesList) {
                    // Not prefix free
                    if (candidate.startsWith(assigned) || assigned.startsWith(candidate)) {
                        isPrefixFree = false;
                        break;
                    }
                }
                if (isPrefixFree) {
                    // Found usable code
                    codeMap.put(targetSymbol, candidate);
                    assignedCodesList.add(candidate);
                    foundCodeForTarget = true;
                    break;
                } else {
                    // If fails prefix, but aside for a little
                    skipped.offerLast(candidate);
                }
            }
            // Put skipped back into pool (maybe used later)
            while (!skipped.isEmpty()) {
                poolDeque.offerLast(skipped.pollFirst());
            }
        }

        // 2. Generate new codes if needed
        // If no fitting code was in pool, create new candidates
        if (!foundCodeForTarget) {
            while (generatedCount < MaxGeneratedNodes) {
                String codeword;
                // Problem
                if (generatorQueue.isEmpty()) {
                    System.out.println("Error: Code generator queue empty????");
                    return null;
                }
                // Dequeue next shortest prefix
                String current = generatorQueue.pollFirst();
                for (int i = 0; i < r; i++) {
                    generatorQueue.offerLast(current + i);
                }
                // Use dequeued item as current candidate
                codeword = current;

                generatedCount++;
                long currentCost = calculateCost(codeword, costs);
                // Invalid cost
                if (currentCost == Long.MAX_VALUE) {
                    continue;
                }
                // Check if cost matches target depth
                if (currentCost == targetDepth) {
                    boolean isPrefixFree = true;
                    // Check if prefix free
                    for (String assigned : assignedCodesList) {
                        if (codeword.startsWith(assigned) || assigned.startsWith(codeword)) {
                            isPrefixFree = false;
                            break;
                        }
                    }
                    if (isPrefixFree) {
                        // Found code
                        codeMap.put(targetSymbol, codeword);
                        assignedCodesList.add(codeword);
                        foundCodeForTarget = true;
                        break;
                    } else {
                        // Correct cost but not prefix free, maybe used later -> put in pool
                        candidatePool.computeIfAbsent((int) currentCost, k -> new ArrayDeque<>())
                                .offerLast(codeword);
                    }
                } else {
                    candidatePool.computeIfAbsent((int) currentCost, k -> new ArrayDeque<>()).offerLast(codeword);
                }
            }

            if (!foundCodeForTarget && generatedCount >= MaxGeneratedNodes) {
                System.out.println(
                        "Error: Max generation limit (" + MaxGeneratedNodes + ") reached for depth " + targetDepth);
                return null;
            }
        }

        if (!foundCodeForTarget) {
            System.out.println(
                    "FATAL: Failed to find code for symbol " + targetSymbol + "' depth " + targetDepth);
            return null;
        }

    }

    // Final check if all symbols received codes
    if (codeMap.size() == n) {
        return codeMap;
    } else {
        System.out.println(
                "Error: Assignment loop finished but only assigned " + codeMap.size() + "/" + n + " codes");
        return null;
    }
}
\end{lstlisting}
\textbf{Kleinere Hilfsmethoden die oben verwendet werden}
\begin{lstlisting}
// Apply reduce operation to signature
// Ensures that signature represents at most n leaves
private static Signature reduceSignature(Signature signature, int n) {
    int m = signature.m;
    int C = signature.getC();

    // Ensure m is not exceeding n to begin with
    int mPrime = Math.min(m, n);
    List<Integer> lsPrime = new ArrayList<>(Collections.nCopies(C, 0)); // Init with zeros
    long currentSum = mPrime; // Had overflow problem with int

    // Iterate through levels l_1, l_2 ..., l_C
    for (int j = 0; j < C; j++) { // j maps to l_{j+1}
        int l_j_plus_1 = signature.getL(j); // ls.get(j)

        // Ensure not exceeding n leaves
        long remainingCapacity = n - currentSum;
        // Get new count
        int l_prime_j_plus_1 = (int) Math.max(0, Math.min(l_j_plus_1, remainingCapacity));

        lsPrime.set(j, l_prime_j_plus_1);
        currentSum += l_prime_j_plus_1;
    }
    return new Signature(mPrime, lsPrime);
}

// Calculate next raw signature before reduction
private static Signature getNextSigRaw(Signature signature, int q, int[] d, int C) {
    int m = signature.m;
    int dLen = d.length; // d represent costs 1 to C

    int l1 = signature.getL(0); // l1 is at index 0
    int mPrime = m + l1 - q;
    List<Integer> lsPrime = new ArrayList<>(Collections.nCopies(C, 0)); // Initit with zeros

    // 1. Shift l's
    for (int k = 0; k < C - 1; k++) { // k from 0 to C-2 relates to l'_1 to l'_{C-1}
        lsPrime.set(k, signature.getL(k + 1));
    }

    // 2. Add new leaves by expanding q nodes
    // Based on characteristic vector
    for (int k = 0; k < C; k++) { // k from 0 to C-1 relates to d_1 to d_C
        if (k < dLen) { // Check bounds for d (d has length C)
            int currentVal = lsPrime.get(k);
            lsPrime.set(k, currentVal + (q * d[k]));
        }
    }
    return new Signature(mPrime, lsPrime);
}

// Calculate cost of one codeword
private static long calculateCost(String codewordStr, List<Integer> costs) {
    long cost = 0;
    int r = costs.size();
    for (int i = 0; i < codewordStr.length(); i++) {
        char digitChar = codewordStr.charAt(i);
        int digit = Character.getNumericValue(digitChar);
        if (digit >= 0 && digit < r) {
            cost += costs.get(digit);
        } else {
            // Invalid
            System.out.println(
                    "Warning: Invalid digit '" + digitChar + "' in codeword '" + codewordStr
                            + "', at this point might aswell just quit");
            return Long.MAX_VALUE;
        }
    }
    return cost;
}
\end{lstlisting}

\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/2.png}
    \caption*{Image 1 caption} % use \caption{} if you want numbered
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/2.png}
    \caption*{Image 2 caption}
  \end{minipage}
  \caption{Comparison of Image 1 and Image 2}
  \label{fig:comparison2}
\end{figure}

\subsection{Vergleich der Strategien}

\begin{thebibliography}{9}
\bibitem{paperhuffman}
David A. Huffman (1952) \emph{A Method for the Construction of Minimum-Redundancy Codes}, 
\emph{Proceedings of the IRE}, 40(9), 1098--1101. 
\url{https://doi.org/10.1109/JRPROC.1952.273898}

\bibitem{papergolinli}
Mordecai Golin and Li Jian (2007) \emph{More Efficient Algorithms and Analyses for Unequal Letter Cost Prefix-Free Coding}, 
arXiv:0705.0253 [cs.IT]. \url{https://doi.org/10.48550/arXiv.0705.0253}

\bibitem{papergolinrote}
M. J. Golin and G. Rote (1998) \emph{A dynamic programming algorithm for constructing optimal prefix-free codes with unequal letter costs}, 
\emph{IEEE Transactions on Information Theory}, 44(5), 1770--1781. 
\url{https://doi.org/10.1109/18.705558}


\end{thebibliography}


\end{document}
